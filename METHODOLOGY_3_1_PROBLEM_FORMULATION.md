# 3.1 问题建模与形式化

## 3.1.1 网约车调度问题的背景与挑战

### 现实背景

网约车服务（ride-hailing）已成为现代城市交通的重要组成部分。在北京、上海等一线城市，每天有数百万的出行需求。网约车平台需要在以下约束条件下做出实时决策：

1. **实时性**：订单到达是随机的，系统需要在毫秒级别做出匹配和调度决策
2. **规模性**：数千辆车、数万个待匹配订单同时存在
3. **动态性**：客流量随时间变化（工作日vs周末、早晚高峰等）
4. **复杂性**：多个相互冲突的目标需要权衡

### 核心调度问题

网约车调度的本质是一个**动态车辆调度问题** (Dynamic Vehicle Routing Problem, DVRP)，其目标是：

$$\max \sum_i (\text{完成订单} \cdot \text{收入}) - \text{乘客等待时间} - \text{系统成本}$$

但这个目标函数过于复杂，难以直接优化。实际上，我们需要关注几个关键指标：

**1. 订单完成率** (Order Completion Rate)

$$\text{Completion Rate} = \frac{\text{完成的订单数}}{\text{生成的订单总数}}$$

这个指标反映了系统的**可靠性**。如果完成率太低，乘客会流失到竞争对手。

**完成率低的原因**：
- 订单因等待超时而被取消
- 没有足够的车辆供应
- 调度策略不当，车辆分布不合理

**2. 订单匹配率** (Order Matching Rate)

$$\text{Matching Rate} = \frac{\text{被成功匹配的订单数}}{\text{生成的订单总数}}$$

这个指标反映了系统的**响应能力**。匹配率高意味着大多数订单能够快速找到车辆。

**匹配率低的原因**：
- 供需失衡（某些区域订单多但车辆少）
- 匹配算法不够优化
- 车辆调度策略导致车辆分布不合理

**3. 平均等待时间** (Average Waiting Time)

$$\text{Avg Waiting Time} = \frac{1}{N} \sum_{i=1}^{N} w_i$$

其中 $w_i$ 是第 $i$ 个订单的等待时间。这个指标反映了**乘客体验**。

等待时间包括两部分：
- **匹配等待时间** ($t_{\text{match}}$)：从下单到被匹配的时间
- **接驾等待时间** ($t_{\text{travel}}$)：从匹配到司机到达的时间

总等待时间：$w_i = t_{\text{match}} + t_{\text{travel}}$

**等待时间长的危害**：
- 乘客不满意，可能取消订单
- 竞争力下降，用户流失
- 平台口碑受损

### 调度的关键挑战

**挑战 1：状态空间爆炸**

在任何给定时刻，系统的状态包括：
- 每个订单的位置、目的地、时间戳
- 每辆车的位置、状态、目前的订单
- 交通状况、天气等环境因素

这导致状态空间的大小为 $O(N^M)$（$N$ 是可能的位置数，$M$ 是车辆和订单数），是天文数字。

**传统方法的局限**：
- 表格型强化学习（Q-learning）：无法处理如此大的状态空间
- 启发式算法（如贪心匹配）：不能自适应地学习最优策略

**我们的解决方案**：使用深度神经网络来近似 Q 函数，将状态空间从 $O(N^M)$ 压缩到神经网络的参数空间。

**挑战 2：多目标冲突**

不同的目标往往相互冲突：

| 目标 | 冲突 | 例子 |
|------|------|------|
| 最大化完成率 | vs | 最小化等待时间 |
| 最大化收入 | vs | 最小化等待时间 |
| 快速完成订单 | vs | 充分利用车辆 |
| 短期收益 | vs | 长期系统稳定性 |

例如，为了最小化等待时间，我们可能会让每辆车只服务最近的订单。但这会导致某些远处的订单永远无法被匹配，完成率下降。

**我们的解决方案**：设计一个**多阶段奖励函数**，在订单的不同生命周期阶段给予不同的反馈，从而权衡这些目标。

**挑战 3：稀疏奖励**

如果只在订单完成时给予奖励，那么在订单被匹配之前，模型无法获得任何反馈。这会导致：

1. **学习信号稀疏**：在一个 2 天的 episode 中，可能有数万个订单，但大多数时间步没有反馈
2. **学习缓慢**：模型需要通过"运气"发现好的策略，然后才能获得正反馈
3. **学习不稳定**：稀疏的反馈导致梯度信号不稳定

**我们的解决方案**：在订单的三个关键阶段（匹配、完成、取消）都给予反馈，使得学习信号更密集。

**挑战 4：非平稳环境**

网约车的客流量随时间变化：

- **小时级别**：早上 7-9 点和晚上 5-7 点是高峰
- **日级别**：工作日和周末的流量不同
- **季节级别**：不同季节的出行需求不同
- **事件级别**：特殊事件（如大型活动）会突然改变流量

这意味着"最优策略"也在不断变化。一个在早上高峰学到的策略可能不适用于晚上。

**我们的解决方案**：
1. 在状态中添加时间特征（time_sin, time_cos）
2. 添加日期编码（day of week）
3. 持续训练，使模型能够适应环境变化

---

## 3.1.2 马尔可夫决策过程 (MDP) 的形式化

### MDP 的定义

我们将网约车调度问题形式化为一个**离散时间的马尔可夫决策过程** (Markov Decision Process, MDP)，记为 $\langle \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma \rangle$。

**MDP 的五个要素**：

1. **状态空间** $\mathcal{S}$：系统在任何时刻可能处于的所有状态的集合
2. **动作空间** $\mathcal{A}$：agent 在任何状态下可以采取的所有动作的集合
3. **状态转移概率** $\mathcal{P}$：描述采取某个动作后系统如何转移到下一个状态
4. **奖励函数** $\mathcal{R}$：描述采取某个动作后 agent 获得多少奖励
5. **折扣因子** $\gamma \in [0, 1]$：平衡近期和远期奖励的权重

**马尔可夫性质**：

MDP 假设系统满足**马尔可夫性质** (Markov property)：

$$P(s_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, ..., s_0, a_0) = P(s_{t+1} | s_t, a_t)$$

即，下一个状态只依赖于当前状态和当前动作，不依赖于历史。

**在网约车调度中的含义**：
- 当前的订单分布、车辆分布、时间信息足以确定系统的"现状"
- 未来的转移只依赖于当前的状态和调度决策
- 历史的调度决策不再重要（除非它们影响了当前状态）

这个假设在大多数情况下是合理的，但有一些细微的例外（如车辆的行驶惯性），我们通过在状态中包含车辆速度等信息来近似处理。

### 地理空间的离散化

#### 网格划分的必要性

城市的地理空间是连续的，但为了使问题可计算，我们需要将其离散化。

**为什么不用连续坐标？**

1. **计算复杂性**：连续空间的状态空间是无限的，无法用神经网络的有限参数表示
2. **学习效率**：两个距离很近的位置应该有相似的策略，但在连续空间中这种相似性不明显
3. **实际性**：实际的网约车平台通常使用网格或热点的概念

**网格划分的优点**：

1. **可计算性**：有限的状态空间，可以用神经网络表示
2. **空间相关性**：相邻网格有自然的邻接关系，便于图卷积网络学习
3. **实际性**：符合实际平台的热点划分方式

#### 网格的具体设置

我们将城市划分为一个 **20 × 20** 的网格，共 **400** 个网格单元。

**网格大小的选择**：

- **太粗（如 5×5 = 25 网格）**：
  - 优点：状态空间小，计算快
  - 缺点：信息丢失，无法区分同一网格内的不同位置

- **太细（如 50×50 = 2500 网格）**：
  - 优点：信息详细
  - 缺点：状态空间太大，学习困难，计算慢

- **20×20 = 400 网格**（我们的选择）：
  - 平衡了信息和计算复杂性
  - 在北京这样的城市中，每个网格大约是 1-2 km²
  - 这个大小足以区分不同的地理位置，但又不会导致状态空间过大

#### 网格坐标的计算

每个网格 $g_i$ ($i \in \{0, 1, ..., 399\}$) 的地理中心坐标为：

$$\text{row} = \lfloor i / 20 \rfloor$$
$$\text{col} = i \bmod 20$$

$$\text{Latitude}_i = \text{Lat}_{\min} + (\text{row} + 0.5) \cdot \Delta \text{Lat}$$
$$\text{Longitude}_i = \text{Lon}_{\min} + (\text{col} + 0.5) \cdot \Delta \text{Lon}$$

其中 $\Delta \text{Lat}$ 和 $\Delta \text{Lon}$ 是网格的纬度和经度跨度。

例如，在北京（约 39.9°N, 116.4°E），一个 20×20 的网格覆盖约 1.2° × 1.2°，每个网格约 1-2 km²。

### 时间的离散化

#### 为什么需要离散化？

时间也是连续的，但为了与状态空间和动作空间匹配，我们需要将其离散化。

**离散时间步的定义**：

我们将时间离散为 **Ticks**，每个 Tick 对应 **30 秒**的真实时间。

**为什么选择 30 秒？**

- **太短（如 1 秒）**：
  - 每天有 86,400 个时间步，计算量太大
  - 在 1 秒内，车辆的位置变化很小，状态变化不大

- **太长（如 10 分钟）**：
  - 每天只有 144 个时间步，信息损失太多
  - 在 10 分钟内，可能有很多订单到达和完成，无法准确模拟

- **30 秒**（我们的选择）：
  - 每天有 $\frac{24 \times 60 \times 60}{30} = 2880$ 个时间步
  - 这个时间尺度足以捕捉订单的动态变化
  - 计算量可以接受

#### 一个 Episode 的时间长度

我们设定每个训练 episode 的长度为 **2 天**，即：

$$\text{MAX\_TICKS\_PER\_EPISODE} = 2 \times 2880 = 5760 \text{ ticks}$$

**为什么选择 2 天？**

- **太短（如 1 小时）**：
  - 无法捕捉完整的日常周期（早晚高峰）
  - 模型无法学到长期的策略

- **太长（如 1 周）**：
  - 每个 episode 太长，训练时间太长
  - 梯度更新不够频繁

- **2 天**（我们的选择）：
  - 包含两个完整的早晚高峰
  - 足以让模型学到基本的调度规律
  - 训练时间可以接受（约 1-2 小时）

### 时间特征的编码

#### 问题：直接使用时间值的缺陷

如果我们直接将时间值（如 0-23 表示小时）作为特征，会有几个问题：

1. **边界不连续**：23:59 和 00:00 在时间上相邻，但在数值上相差 23
2. **周期性丢失**：神经网络难以理解时间的循环特性
3. **尺度问题**：小时（0-23）和分钟（0-59）的尺度不同

#### 解决方案：三角函数编码

我们使用**三角函数编码**来表示时间的周期性：

$$\text{time\_sin} = \sin\left(\frac{2\pi \cdot t}{T}\right)$$
$$\text{time\_cos} = \cos\left(\frac{2\pi \cdot t}{T}\right)$$

其中 $t$ 是当前时刻（以分钟或小时计），$T$ 是周期（如 24 小时）。

#### 三角函数编码的优点

**1. 周期性**：

时间被映射到单位圆上的一个点，自动满足周期性：

$$\sin(t) = \sin(t + 2\pi) = \sin(t + T)$$

**2. 连续性**：

相邻的时刻对应单位圆上相邻的点，保证了连续性：

$$\lim_{\Delta t \to 0} (\sin(t + \Delta t), \cos(t + \Delta t)) = (\sin(t), \cos(t))$$

**3. 距离度量**：

两个时刻的"相似度"可以用欧几里得距离度量：

$$d(t_1, t_2) = \sqrt{(\sin(t_1) - \sin(t_2))^2 + (\cos(t_1) - \cos(t_2))^2}$$

这个距离在时间上是对称的，反映了时间的循环特性。

#### 具体的编码方式

在我们的实现中，对于当前时刻 $t$（以分钟计，范围 0-1439），我们计算：

$$\text{time\_sin} = \sin\left(\frac{2\pi \cdot t}{24 \times 60}\right) = \sin\left(\frac{2\pi \cdot t}{1440}\right)$$
$$\text{time\_cos} = \cos\left(\frac{2\pi \cdot t}{24 \times 60}\right) = \cos\left(\frac{2\pi \cdot t}{1440}\right)$$

这样，时间被编码为单位圆上的一个点。

**具体例子**：

| 时刻 | time_sin | time_cos | 含义 |
|------|----------|----------|------|
| 00:00 | 0.0 | 1.0 | 午夜 |
| 06:00 | 1.0 | 0.0 | 早晨 |
| 12:00 | 0.0 | -1.0 | 正午 |
| 18:00 | -1.0 | 0.0 | 傍晚 |

---

## 3.1.3 状态空间 (State Space)

### 完整的状态定义

在时刻 $t$，系统的状态 $s_t \in \mathcal{S}$ 定义为一个张量：

$$s_t = \{\mathbf{n}_t, \mathbf{v}_t^{\text{idle}}, \mathbf{v}_t^{\text{busy}}, \text{time\_sin}_t, \text{time\_cos}_t\}$$

其中：

- $\mathbf{n}_t \in \mathbb{R}^{400}_{\geq 0}$：每个网格的待匹配订单数量向量
- $\mathbf{v}_t^{\text{idle}} \in \mathbb{R}^{400}_{\geq 0}$：每个网格的空闲车辆数量向量
- $\mathbf{v}_t^{\text{busy}} \in \mathbb{R}^{400}_{\geq 0}$：每个网格的执行任务的车辆数量向量
- $\text{time\_sin}_t \in [-1, 1]$：时间特征（sin 分量）
- $\text{time\_cos}_t \in [-1, 1]$：时间特征（cos 分量）

### 状态的形状和维度

状态的形状为 $(400, 5)$，即：

$$s_t \in \mathbb{R}^{400 \times 5}$$

总维度为 $400 \times 5 = 2000$。

在批处理中，当同时处理 $B$ 个样本时，状态的形状为 $(B, 400, 5)$。

### 每个分量的物理含义

#### 订单数 $\mathbf{n}_t$

$n_i(t)$ 表示在时刻 $t$，网格 $i$ 中有多少个订单正在等待被匹配。

**变化规律**：

- **增加**：当新订单到达网格 $i$ 时，$n_i(t)$ 增加 1
- **减少**：当网格 $i$ 中的订单被匹配时，$n_i(t)$ 减少 1
- **消失**：当订单因等待超时而被取消时，$n_i(t)$ 减少 1

**数值范围**：

在正常情况下，$n_i(t)$ 通常在 0-100 之间。在高峰期，可能达到 200+。

**含义**：

- $n_i(t)$ 高：网格 $i$ 的客流量大，需要更多的车辆
- $n_i(t)$ 低：网格 $i$ 的客流量小，车辆可能过剩

#### 空闲车数 $\mathbf{v}_t^{\text{idle}}$

$v_i^{\text{idle}}(t)$ 表示在时刻 $t$，网格 $i$ 中有多少辆车处于空闲状态（没有订单，可以立即接单）。

**变化规律**：

- **增加**：
  - 当车辆完成订单后回到网格 $i$ 时
  - 当调度决策将车辆派往网格 $i$ 时

- **减少**：
  - 当空闲车辆被匹配到订单时
  - 当调度决策将车辆派往其他网格时

**数值范围**：

在正常情况下，$v_i^{\text{idle}}(t)$ 通常在 1-50 之间。

**含义**：

- $v_i^{\text{idle}}(t)$ 高：网格 $i$ 的车辆供应充足，可以快速响应订单
- $v_i^{\text{idle}}(t)$ 低：网格 $i$ 的车辆供应不足，可能导致订单无法被匹配

#### 繁忙车数 $\mathbf{v}_t^{\text{busy}}$

$v_i^{\text{busy}}(t)$ 表示在时刻 $t$，网格 $i$ 中有多少辆车正在执行订单（已接单但尚未完成）。

**变化规律**：

- **增加**：当车辆在网格 $i$ 接单后立即增加
- **减少**：当车辆在网格 $i$ 完成订单后减少

**数值范围**：

在正常情况下，$v_i^{\text{busy}}(t)$ 通常在 5-50 之间。

**含义**：

- $v_i^{\text{busy}}(t)$ 高：网格 $i$ 的车辆都在忙碌，可能导致新订单无法快速被接单
- $v_i^{\text{busy}}(t)$ 低：网格 $i$ 的车辆利用率不足

#### 供需比例

一个重要的衍生指标是**供需比例**：

$$\text{Supply-Demand Ratio} = \frac{v_i^{\text{idle}}(t)}{n_i(t) + \epsilon}$$

其中 $\epsilon$ 是一个小的常数（如 0.1），用于避免除以零。

- 比例 > 1：车辆供应充足，大多数订单能快速被匹配
- 比例 ≈ 1：供需基本平衡
- 比例 < 1：车辆供应不足，订单无法快速被匹配

### 状态的完整性和可观测性

#### 完整性

状态 $s_t$ 包含了做出调度决策所需的所有关键信息：

1. **订单信息**：$\mathbf{n}_t$ 告诉我们哪些网格有订单需要被匹配
2. **车辆信息**：$\mathbf{v}_t^{\text{idle}}$ 和 $\mathbf{v}_t^{\text{busy}}$ 告诉我们哪些网格有空闲车辆
3. **时间信息**：time_sin 和 time_cos 告诉我们当前是什么时刻

这些信息足以让 agent 判断：
- 哪些网格需要调度
- 应该调度多少车辆
- 调度到哪个热点

#### 可观测性

状态 $s_t$ 中的所有信息都可以从实时系统中直接观测到：

- 订单数：来自待匹配订单队列
- 空闲车数：来自车辆状态数据库
- 繁忙车数：来自活跃订单数据库
- 时间：来自系统时钟

因此，这个状态定义是**完全可观测的** (fully observable)。

---

## 3.1.4 动作空间 (Action Space)

### 动作的定义

在时刻 $t$，agent 选择一个动作 $a_t \in \mathcal{A}$，其中：

$$\mathcal{A} = \{0, 1, 2, ..., 178\}$$

共 **179** 个离散动作。

每个动作 $a$ 对应一个**热点网格位置** $g_a$。

### 动作的解释

当 agent 在时刻 $t$ 选择动作 $a$ 时，系统的解释为：

**"将一定数量的空闲车辆调度到热点网格 $g_a$"**

具体的调度过程：

1. **识别可调度的车辆**：系统识别所有处于空闲状态的车辆
2. **计算目标数量**：根据网格 $g_a$ 的客流量和当前车辆数，计算应该调度多少车辆
3. **派遣车辆**：系统将选定的车辆派往网格 $g_a$，车辆沿着最短路径行驶

### 为什么是 179 个动作而不是 400 个？

虽然城市被划分为 400 个网格，但动作空间只有 179 个。这背后有深层的原因。

#### 热点识别

通过对真实数据的分析，我们发现：

- **179 个网格**：具有足够的客流量，值得作为调度目标
- **221 个网格**：客流量很少，调度到这些位置的车辆难以获得订单

**具体的数据**：

假设我们统计了一个月的数据，计算每个网格的平均订单到达率：

| 网格类型 | 数量 | 平均到达率 | 特征 |
|---------|------|-----------|------|
| 高流量 | 50 | > 10 orders/hour | 商业中心、交通枢纽 |
| 中流量 | 129 | 1-10 orders/hour | 住宅区、办公区 |
| 低流量 | 221 | < 1 order/hour | 偏远地区、工业区 |

我们将高流量和中流量的网格作为热点，共 179 个。

#### 计算效率

将动作空间从 400 减少到 179，有以下好处：

**1. Q 值表大小的减少**

在表格型强化学习中，Q 值表的大小为 $|S| \times |A|$。虽然我们使用的是函数近似（神经网络），但最后一层的输出维度仍然是 $|A|$。

- 400 个动作：最后一层有 400 个输出神经元
- 179 个动作：最后一层有 179 个输出神经元

这减少了约 55% 的参数量。

**2. 推理速度的提升**

推理时间与输出维度成正比：

- 400 个动作：需要计算 400 个 Q 值
- 179 个动作：需要计算 179 个 Q 值

这提升了约 55% 的推理速度，对于实时系统很重要。

**3. 训练数据的效率**

较小的动作空间意味着：
- 每个动作获得更多的训练数据
- 模型对每个动作的 Q 值估计更准确
- 收敛更快

#### 学习效率

从强化学习的角度，较小的动作空间也有好处：

**1. 减少探索空间**

在探索阶段，agent 需要尝试不同的动作来学习它们的效果。较小的动作空间意味着：
- 需要探索的动作更少
- 每个动作被探索的频率更高
- 更快地发现最优动作

**2. 减少高估偏差**

在 DQN 中，高估偏差与动作空间的大小有关。较小的动作空间意味着：
- 高估的可能性更低
- 学习更稳定

### 实际意义

这 179 个热点对应于城市中的关键位置：

- **商业中心**：如购物商场、商业街
- **交通枢纽**：如火车站、地铁站、机场
- **住宅区**：如大型居住社区
- **办公区**：如商务区、产业园
- **景点**：如公园、博物馆、体育场

这些位置都是高客流量的地方，也是最有价值的调度目标。

---

## 3.1.5 奖励函数的设计

### 为什么需要精心设计奖励函数？

在强化学习中，**奖励函数是学习信号的唯一来源**。agent 的所有行为都是在最大化累积奖励的驱动下进行的。

不同的奖励函数会导致完全不同的学习行为和最终的策略。

#### 简单奖励的问题

假设我们使用最简单的奖励：每完成一个订单得到 1 分。

$$R_t = \text{完成的订单数}$$

这会导致什么问题？

**问题 1：忽视乘客体验**

一个完成一个订单需要 5 分钟和 50 分钟得到的奖励相同，都是 1 分。

因此，agent 没有动机减少乘客等待时间。它可能采取低效的调度策略，导致平均等待时间很长，乘客满意度低。

**问题 2：短期行为**

agent 会优先完成那些容易完成的订单（如距离近的），而不是优化长期的系统效率。

例如，如果某个偏远地区有很多订单等待，但距离远，agent 可能会忽视这些订单，而是集中在容易完成的订单上。

**问题 3：信号稀疏**

如果只在订单完成时给予奖励，中间的调度决策无法获得反馈。

在一个 2 天的 episode 中，可能有数万个订单，但大多数时间步没有任何订单完成，因此没有奖励信号。

这导致学习信号稀疏，模型难以学到因果关系（"这个调度决策导致了之后的订单完成"）。

**问题 4：多目标冲突**

实际上，系统有多个目标：

- 最大化完成率
- 最小化等待时间
- 最大化收入
- 保持系统稳定

单一的奖励信号无法权衡这些目标。

### 多阶段奖励函数的设计思想

我们的解决方案是设计一个**多阶段奖励函数**，在订单的不同生命周期阶段给予不同的反馈。

**订单的生命周期**：

```
下单
  ↓
等待匹配 ← 阶段 1：匹配阶段
  ↓
被匹配
  ↓
等待接驾
  ↓
司机到达
  ↓
乘坐
  ↓
完成 ← 阶段 2：完成阶段
  ↓
结束

或者

等待超时
  ↓
取消 ← 阶段 3：取消阶段
  ↓
结束
```

在每个关键阶段，我们给予相应的奖励或惩罚。

### 阶段 1：匹配阶段

**定义**：当一个待匹配的订单成功与一辆空闲车匹配时

**奖励**：

$$R_{\text{match}} = W_{\text{match}} \cdot 1 = 1.2$$

**为什么给予这个奖励？**

1. **及时激励**：在订单被匹配的瞬间给予奖励，agent 能立即获得反馈
2. **提高匹配率**：通过奖励匹配事件，鼓励 agent 采取能提高匹配率的调度策略
3. **密集信号**：即使订单最终没有完成，只要被匹配就有奖励，学习信号更密集

**权重的选择** ($W_{\text{match}} = 1.2$)：

这个权重是通过以下方式确定的：

1. **初始设置**：基于业务直觉，匹配一个订单应该得到一定的奖励
2. **实验调整**：训练多个模型，使用不同的权重（如 0.5, 1.0, 1.5, 2.0）
3. **性能评估**：评估每个权重下的模型性能（完成率、匹配率、等待时间）
4. **权衡**：选择能够平衡所有指标的权重

权重不能太高（否则 agent 会过度优化匹配率，忽视等待时间），也不能太低（否则 agent 会忽视匹配的重要性）。

### 阶段 2：完成阶段

**定义**：当一个订单被司机成功完成时

**奖励**：

$$R_{\text{completion}} = W_{\text{completion}} \cdot \exp\left(-\frac{w_t}{T_0}\right) = 2.0 \cdot \exp\left(-\frac{w_t}{240}\right)$$

其中：

- $W_{\text{completion}} = 2.0$ 是完成奖励的权重
- $w_t$ 是乘客的**理论等待时间**（单位：秒）
- $T_0 = 240$ 秒是特征时间常数

#### 理论等待时间的定义

$$w_t = t_{\text{match}} + t_{\text{travel}}$$

其中：

- $t_{\text{match}}$：从乘客下单到被匹配的时间（秒）
- $t_{\text{travel}}$：司机从当前位置到达乘客位置的预计时间（秒）

**为什么这样定义？**

1. **准确性**：它准确反映了乘客的真实体验
   - 乘客关心的是从下单到司机到达的总时间
   - 不仅仅是匹配等待时间

2. **完整性**：它包含了调度决策的所有影响
   - 匹配等待时间取决于系统的供需平衡（调度决策的结果）
   - 接驾时间取决于司机和乘客的距离（也与调度决策有关）

3. **可预测性**：$t_{\text{travel}}$ 可以从距离和平均速度预测
   - 距离 = Haversine 公式计算的地理距离
   - 平均速度 = 40 km/h（配置参数）
   - 预计时间 = 距离 / 速度

#### 指数衰减函数的特性

指数衰减函数 $\exp(-w_t / T_0)$ 有以下特性：

**当 $w_t = 0$ 时**：$\exp(0) = 1.0$（最大奖励）

**当 $w_t = T_0 = 240$ s 时**：$\exp(-1) \approx 0.368$（约 37% 的奖励）

**当 $w_t = 2T_0 = 480$ s 时**：$\exp(-2) \approx 0.135$（约 14% 的奖励）

**当 $w_t = 3T_0 = 720$ s 时**：$\exp(-3) \approx 0.050$（约 5% 的奖励）

这个函数的图形：

```
奖励
1.0 |●
    |  ●
0.8 |    ●
    |      ●
0.6 |        ●
    |          ●
0.4 |            ●
    |              ●
0.2 |                ●
    |                  ●
0.0 |_______________________●
    0   240  480  720  960
        等待时间 (秒)
```

**为什么选择指数衰减而不是线性衰减？**

**线性衰减的问题**：

$$R_{\text{linear}} = \max(0, 1 - \frac{w_t}{w_{\max}})$$

- 当 $w_t > w_{\max}$ 时，奖励变为负数，这会导致学习不稳定
- 从 0 到 1 分钟的不满度增加，与从 9 到 10 分钟的增加相同（不符合心理学）

**指数衰减的优点**：

1. **始终非负**：$\exp(-x) > 0$ 对所有 $x$，避免了负奖励的问题
2. **符合心理学**：早期的等待时间增加导致更大的不满度增加
3. **参数化控制**：通过调整 $T_0$，可以灵活控制对等待时间的惩罚力度
4. **物理意义**：这个函数对应于指数分布的累积分布函数

#### 权重的选择 ($W_{\text{completion}} = 2.0$)

这是最大的权重，原因：

1. **重要性**：订单完成是系统的最终目标
2. **质量考量**：完成奖励考虑了等待时间，反映了完成的质量
3. **权衡**：大于匹配奖励（1.2），鼓励系统优先完成订单而不是仅仅匹配

### 阶段 3：取消阶段

**定义**：当一个订单因等待超时而被取消时

**惩罚**：

$$R_{\text{cancel}} = -W_{\text{cancel}} \cdot 1 = -1.0$$

**为什么给予这个惩罚？**

1. **强烈反馈**：订单取消是系统的失败，应该给予强烈的负反馈
2. **避免超时**：通过惩罚，鼓励 agent 优先处理即将超时的订单
3. **提高可靠性**：降低系统的取消率，提高乘客满意度

**超时的定义**：

在我们的系统中，订单的最大等待时间为 **300 秒**（5 分钟）。

如果一个订单在 300 秒内没有被匹配，它会被自动取消。

**权重的选择** ($W_{\text{cancel}} = 1.0$)：

这是一个中等的权重，原因：

1. **严重性**：取消是坏的，但不如完成订单是好的那么重要
2. **平衡**：不能太高（否则 agent 会过度优化避免取消，忽视等待时间），也不能太低（否则 agent 会忽视取消的问题）

### 总奖励函数

每个时间步 $t$ 的总奖励为所有三个阶段奖励的和：

$$R_t = R_{\text{match}}(t) + R_{\text{completion}}(t) + R_{\text{cancel}}(t)$$

其中：

- $R_{\text{match}}(t) = 1.2 \times (\text{在时刻 } t \text{ 新匹配的订单数})$
- $R_{\text{completion}}(t) = \sum_{\text{在时刻 } t \text{ 完成的订单}} 2.0 \cdot \exp(-w_i / 240)$
- $R_{\text{cancel}}(t) = -1.0 \times (\text{在时刻 } t \text{ 取消的订单数})$

**累积奖励**：

在一个 episode（2 天）中，总奖励为：

$$R_{\text{episode}} = \sum_{t=1}^{5760} R_t$$

这个累积奖励是 agent 优化的目标。

### 奖励权重的最终设置

最终的奖励权重设置总结在下表中：

| 权重 | 值 | 作用 | 调整理由 |
|------|-----|------|---------|
| $W_{\text{match}}$ | 1.2 | 鼓励快速匹配 | 平衡匹配率和等待时间 |
| $W_{\text{completion}}$ | 2.0 | 重视订单完成质量 | 最重要的目标 |
| $W_{\text{cancel}}$ | 1.0 | 惩罚订单超时 | 避免系统失败 |
| $T_0$ | 240 s | 特征时间 | 大约 4 分钟的等待是"中等" |

这些权重的设置是通过以下方式得到的：

1. **初始设置**：基于业务目标和直觉
2. **实验调整**：通过多次训练实验，观察模型的学习行为
3. **性能评估**：评估不同权重配置下的系统性能
4. **微调**：根据评估结果进行微调，确保权重能够平衡所有目标

