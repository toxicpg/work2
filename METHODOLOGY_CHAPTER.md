# 第 3 章 方法论

## 3.1 问题建模

### 3.1.1 网约车调度问题的形式化定义

网约车调度问题可以形式化为一个**马尔可夫决策过程 (MDP)**，定义为五元组 $\langle \mathcal{S}, \mathcal{A}, \mathcal{R}, \mathcal{P}, \gamma \rangle$，其中：

- $\mathcal{S}$ 是状态空间
- $\mathcal{A}$ 是动作空间
- $\mathcal{R}$ 是奖励函数
- $\mathcal{P}$ 是状态转移概率
- $\gamma \in [0,1]$ 是折扣因子

#### 地理环境建模

我们将城市地理区域离散化为一个 $20 \times 20$ 的网格，共 400 个网格单元。每个网格单元 $g_i$ ($i=1,2,...,400$) 代表一个地理位置。这种网格化表示方法具有以下优点：

1. **计算效率**：相比于连续坐标表示，网格表示显著降低了状态空间复杂度
2. **空间相关性**：相邻网格具有自然的空间邻接关系，便于图卷积网络的学习
3. **实际应用**：符合实际网约车平台的热点网格划分方式

#### 时间建模

模拟器采用**离散时间步** (discrete time steps) 的方式进行时间推进。每个时间步 (Tick) 对应 30 秒的真实时间，因此每天有 $\lfloor 24 \times 60 \times 60 / 30 \rfloor = 2880$ 个时间步。

为了捕捉时间的周期性特征（小时、分钟），我们使用**三角函数编码** (trigonometric encoding)：
$$\text{time\_sin} = \sin\left(\frac{2\pi \cdot t}{T}\right), \quad \text{time\_cos} = \cos\left(\frac{2\pi \cdot t}{T}\right)$$

其中 $t$ 是当前时刻，$T$ 是周期（如 24 小时）。这种编码方式能够自然地表示时间的循环性质，避免了时间边界处的不连续性。

### 3.1.2 状态空间 (State Space)

**状态 $s_t \in \mathcal{S}$ 在时刻 $t$ 的定义**为：

$$s_t = \{g_i^{\text{orders}}, g_i^{\text{idle}}, g_i^{\text{busy}}, \text{time\_sin}, \text{time\_cos}\}_{i=1}^{400}$$

其中：

- $g_i^{\text{orders}}$ ∈ $\mathbb{R}^+$：网格 $i$ 中**待匹配的订单数量**
- $g_i^{\text{idle}}$ ∈ $\mathbb{R}^+$：网格 $i$ 中**空闲车辆的数量**
- $g_i^{\text{busy}}$ ∈ $\mathbb{R}^+$：网格 $i$ 中**执行任务的车辆数量**
- $\text{time\_sin}, \text{time\_cos}$ ∈ $[-1, 1]$：**时间特征**

因此，状态向量的维度为 $400 \times 5 = 2000$。这种表示方法具有以下特点：

1. **完整性**：状态包含了调度决策所需的所有关键信息
2. **可观测性**：所有信息都可以从实时系统中直接获取
3. **紧凑性**：相比于记录每个订单和车辆的个体信息，网格聚合表示大幅降低了状态维度

### 3.1.3 动作空间 (Action Space)

**动作 $a_t \in \mathcal{A}$ 代表调度决策**，具体为：

$$a_t \in \{1, 2, ..., 179\}$$

其中每个动作 $a$ 对应一个**热点网格位置** $g_a$。当 agent 选择动作 $a$ 时，系统会将空闲车辆调度到对应的热点网格 $g_a$。

**动作空间的大小为 179 而非 400 的原因**：

1. **热点识别**：通过数据分析，我们识别了城市中的 179 个关键热点位置
2. **计算效率**：相比于 400 个网格，179 个热点显著减少了 Q 值表的大小，提高了学习效率
3. **实际意义**：这些热点对应于客流量高、调度需求大的区域

### 3.1.4 奖励函数 (Reward Function)

**奖励函数是强化学习的核心**，它将业务目标转化为学习信号。我们设计了一个**多阶段奖励函数**，在订单的不同生命周期阶段给予相应的反馈。

#### 问题动机

传统的网约车调度系统通常使用单一的奖励信号（如总收入或完成订单数），这种设计存在以下问题：

1. **目标冲突**：最大化收入可能导致忽视乘客体验（长等待时间）
2. **信号稀疏**：如果只在订单完成时才给予奖励，会导致学习信号稀疏
3. **不完整**：无法区分"快速完成订单"和"缓慢完成订单"的差异

#### 多阶段奖励设计

我们的奖励函数在订单的三个关键阶段给予反馈：

**阶段 1：匹配阶段（订单被成功匹配）**

$$R_{\text{match}} = W_{\text{match}} \cdot 1$$

其中 $W_{\text{match}} = 1.2$ 是匹配奖励的权重。

**设计理由**：
- 鼓励系统快速响应客户需求
- 提高订单匹配率
- 为模型提供及时的正反馈

**阶段 2：完成阶段（订单被司机完成）**

$$R_{\text{completion}} = W_{\text{completion}} \cdot \exp\left(-\frac{w_t}{T_0}\right)$$

其中：
- $W_{\text{completion}} = 2.0$ 是完成奖励的权重
- $w_t$ 是乘客的**理论等待时间**（秒）
- $T_0 = 240$ 秒是特征时间常数

**理论等待时间的定义**：

$$w_t = t_{\text{match}} + t_{\text{travel}}$$

其中：
- $t_{\text{match}}$ 是从下单到被匹配的时间
- $t_{\text{travel}}$ 是司机从当前位置到达乘客位置的预计时间

这个定义准确反映了乘客的真实体验，而不仅仅是匹配等待时间。

**指数衰减函数的特性**：

$$\exp\left(-\frac{w_t}{T_0}\right) = \begin{cases}
\approx 1.0 & \text{if } w_t \ll T_0 \text{ (短等待)} \\
\approx 0.37 & \text{if } w_t = T_0 \text{ (中等等待)} \\
\approx 0 & \text{if } w_t \gg T_0 \text{ (长等待)}
\end{cases}$$

这种设计的优点：

1. **平滑衰减**：避免了阈值函数的突变
2. **参数化**：通过调整 $T_0$，可以灵活控制对等待时间的惩罚力度
3. **物理意义**：对应于指数分布的累积分布函数

**阶段 3：取消阶段（订单因超时被取消）**

$$R_{\text{cancel}} = -W_{\text{cancel}} \cdot 1$$

其中 $W_{\text{cancel}} = 1.0$ 是取消惩罚的权重。

**设计理由**：
- 强烈惩罚系统的失败（订单超时取消）
- 鼓励模型优先处理即将超时的订单
- 提高系统的可靠性

#### 总奖励函数

每个时间步的总奖励为：

$$R_t = R_{\text{match}} + R_{\text{completion}} + R_{\text{cancel}} + R_{\text{wait}}$$

其中 $R_{\text{wait}}$ 是等待时间的额外惩罚项（为了加强对等待时间的约束）。

**奖励权重的设置**：

| 权重 | 值 | 作用 |
|------|-----|------|
| $W_{\text{match}}$ | 1.2 | 鼓励快速匹配 |
| $W_{\text{completion}}$ | 2.0 | 重视订单完成质量 |
| $W_{\text{cancel}}$ | 1.0 | 惩罚订单超时 |
| $W_{\text{wait}}$ | 1.8 | 惩罚长等待时间 |

这些权重是通过多次实验调整得到的，反映了不同目标的相对重要性。

---

## 3.2 深度强化学习模型

本节详细介绍我们提出的深度强化学习模型。该模型是一个端到端的神经网络架构，将网约车调度问题的状态输入转化为最优的调度决策。模型的核心设计理念是：**通过多图卷积网络捕捉空间信息，通过 Dueling DQN 分离值函数和优势函数，从而更稳定和高效地学习调度策略**。

### 3.2.1 总体架构设计

#### 架构的基本组成

我们的模型采用**端到端的深度强化学习框架**，整体流程如下：

```
┌─────────────────────────────────────────────────────────────┐
│  输入状态 s_t: (B, 400, 5)                                   │
│  [订单数, 空闲车, 繁忙车, time_sin, time_cos]                 │
└──────────────────────┬──────────────────────────────────────┘
                       │
        ┌──────────────▼──────────────┐
        │  多图卷积网络 (MGCN)        │
        │  ├─ 邻接图分支 (5→64→32)   │
        │  └─ POI 图分支 (5→64→32)   │
        │  输出: (B, 400, 32)         │
        └──────────────┬──────────────┘
                       │
        ┌──────────────▼──────────────┐
        │  特征融合层                  │
        │  ├─ 全局池化: 32→32         │
        │  ├─ 位置编码: 1→16          │
        │  ├─ 日期编码: 1→8           │
        │  └─ 融合网络: 56→128→64    │
        │  输出: (B, 64)              │
        └──────────────┬──────────────┘
                       │
        ┌──────────────▼──────────────┐
        │  Dueling DQN 头             │
        │  ├─ 值流: 64→128→1         │
        │  └─ 优势流: 64→128→179     │
        │  输出 Q 值: (B, 179)        │
        └──────────────┬──────────────┘
                       │
        ┌──────────────▼──────────────┐
        │  动作选择                    │
        │  a_t = argmax Q(s_t, a)     │
        └──────────────────────────────┘
```

#### 架构的设计原则

我们的模型设计遵循以下几个关键原则：

**原则 1：空间信息的多维度捕捉**

网约车调度涉及复杂的空间关系。不同网格之间既有地理邻接关系（影响交通流动），也有功能相似性（影响客流规律）。单一的图表示无法充分捕捉这些复杂关系，因此我们采用**多图卷积网络**来同时建模这两种关系。

**原则 2：特征的逐层抽象**

模型采用分层的特征抽象策略：
- 第一层：捕捉网格级别的局部特征
- 第二层：学习网格间的相互影响关系
- 融合层：综合全局信息和上下文信息
- 决策层：基于融合特征做出最终决策

**原则 3：值函数和优势函数的分离**

Dueling DQN 的核心思想是分离**"这个状态有多好"**和**"这个动作相对于其他动作有多好"**这两个问题。这种分离能够：
- 减少学习难度，因为值函数不需要依赖于具体的动作
- 提高稳定性，因为优势函数的学习不会被值函数的波动所影响
- 加速收敛，因为值函数的信号会被所有动作共享

---

### 3.2.2 多图卷积网络 (MGCN)

#### 2.2.1 设计动机与理论基础

**为什么需要多图卷积网络？**

在网约车调度问题中，网格之间的关系远比简单的欧几里得距离要复杂。考虑以下两个场景：

**场景 1：空间邻接关系**

假设有两个相邻的网格 $g_i$ 和 $g_{i+1}$，它们地理上相邻。当 $g_i$ 中订单堆积时，调度算法可能会向 $g_{i+1}$ 调度车辆。这种关系反映了**地理邻接性**对调度决策的影响。

**场景 2：功能相似性**

假设有两个不相邻的网格 $g_j$ 和 $g_k$，它们分别代表商业区和另一个商业区。虽然地理位置不同，但它们的 POI 分布相似（都有餐厅、办公楼等），导致它们的客流规律相似。这种关系反映了**功能相似性**对调度决策的影响。

单一的图表示（如仅使用邻接图）只能捕捉第一种关系，而忽视第二种关系。因此，我们设计了**多图卷积网络**来同时建模这两种关系。

**图卷积网络的基本原理**

图卷积网络 (GCN) 的核心思想是：**一个节点的特征是通过聚合其邻接节点的特征来更新的**。标准的 GCN 公式为：

$$H^{(l+1)} = \sigma\left(\sum_{k=0}^{K-1} A_k H^{(l)} W_k^{(l)}\right)$$

这个公式的含义是：
- 对于每个节点 $i$，其第 $l+1$ 层的特征 $h_i^{(l+1)}$ 是通过聚合其邻接节点的第 $l$ 层特征 $h_j^{(l)}$（$j \in \mathcal{N}(i)$）来计算的
- 不同的支撑矩阵 $A_k$ 代表不同的邻接关系（例如，1-hop 邻接、2-hop 邻接等）
- 可学习的权重矩阵 $W_k^{(l)}$ 学习如何组合这些不同的邻接关系

#### 2.2.2 图的定义与构造

**邻接图的构造**

邻接图 $G_{\text{neighbor}} = (\mathcal{V}, \mathcal{E}_{\text{neighbor}})$ 代表网格之间的地理邻接关系：

$$\mathcal{E}_{\text{neighbor}} = \{(i, j) : \text{grid}_i \text{ and } \text{grid}_j \text{ are geographically adjacent}\}$$

在我们的 $20 \times 20$ 网格中，每个内部网格有 8 个邻接网格（包括对角线邻接）。邻接矩阵 $A_{\text{neighbor}} \in \mathbb{R}^{400 \times 400}$ 定义为：

$$A_{\text{neighbor}}[i, j] = \begin{cases}
1 & \text{if } (i, j) \in \mathcal{E}_{\text{neighbor}} \\
0 & \text{otherwise}
\end{cases}$$

为了防止自环的影响过大，我们通常添加自环，得到 $\tilde{A}_{\text{neighbor}} = A_{\text{neighbor}} + I$。

**POI 图的构造**

POI 图 $G_{\text{poi}} = (\mathcal{V}, \mathcal{E}_{\text{poi}})$ 代表网格之间的功能相似性：

$$\mathcal{E}_{\text{poi}} = \{(i, j) : \text{similarity}(\text{POI}_i, \text{POI}_j) > \tau\}$$

其中 $\tau$ 是相似性阈值。POI 相似性通过余弦相似度计算：

$$\text{similarity}(\text{POI}_i, \text{POI}_j) = \frac{\text{POI}_i \cdot \text{POI}_j}{\|\text{POI}_i\| \cdot \|\text{POI}_j\|}$$

这里 $\text{POI}_i \in \mathbb{R}^{c}$ 是网格 $i$ 中各类 POI 的计数向量（$c$ 是 POI 类别数，通常为 10-20）。例如，$\text{POI}_i = [n_{\text{restaurant}}, n_{\text{office}}, n_{\text{hotel}}, ...]$。

在实践中，我们通常使用 $\tau = 0.5$ 作为相似性阈值，这样可以保证每个网格与 10-20 个其他网格相连。

**支撑矩阵的生成**

为了增加 GCN 的表达能力，我们使用多项式支撑矩阵。标准的做法是使用邻接矩阵的多项式组合：

$$A_k = A^k$$

即第 $k$ 个支撑矩阵是邻接矩阵的 $k$ 次幂。$A^k[i,j]$ 代表从节点 $i$ 到节点 $j$ 的 $k$-hop 路径数。

在我们的实现中，我们使用 $K=2$ 个支撑矩阵，即：
- $A_0 = I$（自环）
- $A_1 = \tilde{A}$（1-hop 邻接）

这样可以捕捉节点本身的特征和其直接邻接节点的特征。

#### 2.2.3 分离式多图卷积网络的设计

**为什么采用分离式设计？**

有两种主要的方式来设计多图卷积网络：

**方式 1：联合式设计 (Joint MGCN)**

将所有图的信息在一个卷积层中混合处理：

$$H^{(l+1)} = \sigma\left(\sum_{k=0}^{K_n-1} A_k^{\text{neighbor}} H^{(l)} W_k^{\text{neighbor},(l)} + \sum_{k=0}^{K_p-1} A_k^{\text{poi}} H^{(l)} W_k^{\text{poi},(l)}\right)$$

**方式 2：分离式设计 (Separate MGCN)**

为每种图类型创建独立的卷积分支，然后在最后融合：

$$H_{\text{neighbor}}^{(l+1)} = \sigma\left(\sum_{k=0}^{K_n-1} A_k^{\text{neighbor}} H^{(l)} W_k^{\text{neighbor},(l)}\right)$$

$$H_{\text{poi}}^{(l+1)} = \sigma\left(\sum_{k=0}^{K_p-1} A_k^{\text{poi}} H^{(l)} W_k^{\text{poi},(l)}\right)$$

$$H^{(l+1)} = \text{Fusion}(H_{\text{neighbor}}^{(l+1)}, H_{\text{poi}}^{(l+1)})$$

我们采用**分离式设计**有以下优势：

1. **参数独立性**：每个分支的参数独立学习，避免了不同图信息的相互干扰
2. **可解释性**：可以分别分析邻接图和 POI 图的贡献
3. **灵活性**：可以为不同的分支使用不同的架构或超参数
4. **模块化**：便于进行消融研究，可以轻松移除某个分支

#### 2.2.4 MGCN 的详细实现

**邻接图分支的实现**

邻接图分支的完整实现如下：

```
输入: H^(l) ∈ ℝ^(B×400×d_l)

第 1 步：图卷积操作
  对于 k = 0, 1, ..., K_n-1:
    Z_k = A_k^neighbor @ H^(l) @ W_k^neighbor,(l)
    其中 A_k^neighbor ∈ ℝ^(400×400) 是支撑矩阵
    W_k^neighbor,(l) ∈ ℝ^(d_l × d_{l+1}) 是权重矩阵

第 2 步：特征聚合
  H_neighbor^(l+1) = sum(Z_k for k in 0..K_n-1)

第 3 步：激活函数
  H_neighbor^(l+1) = ReLU(H_neighbor^(l+1))

输出: H_neighbor^(l+1) ∈ ℝ^(B×400×d_{l+1})
```

**POI 图分支的实现**

POI 图分支的实现完全相同，只是使用不同的邻接矩阵 $A^{\text{poi}}$ 和权重矩阵 $W^{\text{poi},(l)}$。

**特征融合策略**

在邻接图分支和 POI 图分支各自完成卷积后，我们需要融合两个分支的特征。我们提供了三种融合方式：

**融合方式 1：拼接融合 (Concatenation)**

$$H_{\text{fused}} = [H_{\text{neighbor}}, H_{\text{poi}}] \in \mathbb{R}^{B \times 400 \times 2d}$$

然后通过线性层将维度从 $2d$ 降低到 $d$：

$$H_{\text{fused}} = H_{\text{fused}} \cdot W_{\text{concat}} + b_{\text{concat}}$$

其中 $W_{\text{concat}} \in \mathbb{R}^{2d \times d}$，$b_{\text{concat}} \in \mathbb{R}^d$。

**优点**：保留了两个分支的所有信息
**缺点**：参数量增加，可能导致过拟合

**融合方式 2：加法融合 (Addition)**

$$H_{\text{fused}} = H_{\text{neighbor}} + H_{\text{poi}}$$

**优点**：参数量最少，计算高效
**缺点**：可能丢失一些信息，特别是当两个分支学到的特征差异很大时

**融合方式 3：注意力融合 (Attention Fusion)** ⭐ 推荐

$$\alpha_{\text{neighbor}}, \alpha_{\text{poi}} = \text{Softmax}(\text{MLP}([H_{\text{neighbor}}, H_{\text{poi}}]))$$

$$H_{\text{fused}} = \alpha_{\text{neighbor}} \odot H_{\text{neighbor}} + \alpha_{\text{poi}} \odot H_{\text{poi}}$$

其中 $\text{MLP}$ 是一个多层感知机，输出两个注意力权重，$\odot$ 表示逐元素乘法。

**优点**：
- 自适应地学习两个分支的重要性权重
- 性能通常最优
- 参数量适中

**缺点**：
- 计算复杂度略高
- 需要更多的训练数据才能有效学习

在我们的实现中，我们采用**注意力融合**，因为它在多个数据集上都显示出最优的性能。

**MGCN 的完整架构**

```
第 0 层（输入层）:
  H^(0) ∈ ℝ^(B×400×5)
  特征: [订单数, 空闲车, 繁忙车, time_sin, time_cos]

第 1 层（GCN 层）:
  邻接图分支: H^(0) → GCN(K=2) → H_neighbor^(1) ∈ ℝ^(B×400×64)
  POI 图分支: H^(0) → GCN(K=2) → H_poi^(1) ∈ ℝ^(B×400×64)
  融合 (注意力): [H_neighbor^(1), H_poi^(1)] → Attention → H^(1) ∈ ℝ^(B×400×64)

第 2 层（GCN 层）:
  邻接图分支: H^(1) → GCN(K=2) → H_neighbor^(2) ∈ ℝ^(B×400×32)
  POI 图分支: H^(1) → GCN(K=2) → H_poi^(2) ∈ ℝ^(B×400×32)
  融合 (注意力): [H_neighbor^(2), H_poi^(2)] → Attention → H^(2) ∈ ℝ^(B×400×32)

输出:
  H^(2) ∈ ℝ^(B×400×32)
  每个网格有一个 32 维的特征向量
```

**参数统计与分析**

让我们详细计算 MGCN 的参数量：

**邻接图分支的参数**：

第 1 层：
- $W_0^{\text{neighbor},(1)} \in \mathbb{R}^{5 \times 64}$：$5 \times 64 = 320$ 参数
- $W_1^{\text{neighbor},(1)} \in \mathbb{R}^{5 \times 64}$：$5 \times 64 = 320$ 参数
- 小计：640 参数

第 2 层：
- $W_0^{\text{neighbor},(2)} \in \mathbb{R}^{64 \times 32}$：$64 \times 32 = 2048$ 参数
- $W_1^{\text{neighbor},(2)} \in \mathbb{R}^{64 \times 32}$：$64 \times 32 = 2048$ 参数
- 小计：4096 参数

邻接图分支总计：**4736 参数**

**POI 图分支的参数**：

同邻接图分支，**4736 参数**

**融合层的参数**：

第 1 层融合：
- 注意力 MLP：$64 \times 2 + 2 = 130$ 参数

第 2 层融合：
- 注意力 MLP：$32 \times 2 + 2 = 66$ 参数

融合层总计：**196 参数**

**MGCN 总参数**：$4736 + 4736 + 196 = 9668 \approx 10K$ 参数

---

### 3.2.3 特征融合层

#### 3.3.1 设计理由与必要性

MGCN 的输出是每个网格的 32 维特征向量，共 400 个网格，即 $(B, 400, 32)$ 的张量。但是，**调度决策是一个全局决策**，而不是针对每个网格的决策。因此，我们需要：

1. **从 400 个网格的特征中提取全局信息**
2. **融合额外的上下文信息**（如当前时间、车辆位置、日期等）
3. **将融合后的信息转化为调度决策所需的特征**

这正是特征融合层的作用。

#### 3.3.2 全局池化 (Global Pooling)

**平均池化的实现**

平均池化是最简单的池化方式：

$$f_{\text{global}} = \frac{1}{400} \sum_{i=1}^{400} H_i^{(2)}$$

这相当于对所有网格的特征取平均值。

**优点**：
- 计算简单，速度快
- 参数量为 0
- 对所有网格等同对待

**缺点**：
- 可能丢失重要的空间信息
- 无法区分不同网格的重要性

**注意力池化的实现** ⭐ 推荐

注意力池化能够自动学习哪些网格对调度决策更重要：

$$\alpha_i = \frac{\exp(\text{MLP}(H_i^{(2)}))}{\sum_{j=1}^{400} \exp(\text{MLP}(H_j^{(2)}))}$$

$$f_{\text{global}} = \sum_{i=1}^{400} \alpha_i H_i^{(2)}$$

其中 $\text{MLP}$ 是一个简单的多层感知机：

```
MLP 的实现:
  输入: H_i^(2) ∈ ℝ^32
    ↓
  线性层: 32 → 16
    ↓
  ReLU 激活
    ↓
  线性层: 16 → 1
    ↓
  输出: 标量 (用于计算注意力权重)
```

**优点**：
- 自适应地学习网格的重要性
- 能够关注高订单量或高车辆密度的网格
- 性能通常优于平均池化

**缺点**：
- 增加了参数量（约 $32 \times 16 + 16 + 16 \times 1 + 1 = 545$ 参数）
- 计算复杂度略高

在我们的实现中，我们采用**注意力池化**。

**池化操作的几何解释**

从几何角度看，池化操作可以理解为：
- **平均池化**：在特征空间中找到所有网格特征的"质心"
- **注意力池化**：找到加权的"质心"，其中权重反映了每个网格的重要性

#### 3.3.3 位置编码 (Position Embedding)

**为什么需要位置编码？**

虽然 MGCN 已经编码了所有网格的信息，但在某些情况下，**当前的参考位置**对调度决策也很重要。例如：

- 如果当前司机在城市的南边，调度算法可能更倾向于向南边的网格调度其他车辆
- 如果当前订单集中在北边，调度算法应该优先向北边调度车辆

因此，我们添加一个位置编码，将当前的参考位置（例如，当前调度的司机所在的位置）作为额外的上下文。

**位置编码的实现**

$$f_{\text{pos}} = \text{Embedding}(\text{vehicle\_location})$$

其中：
- $\text{vehicle\_location} \in \{0, 1, ..., 399\}$ 是当前车辆所在的网格 ID
- Embedding 层是一个查找表，将网格 ID 映射到一个 16 维的向量

从数学角度，Embedding 层是一个矩阵 $E_{\text{pos}} \in \mathbb{R}^{400 \times 16}$，其中第 $i$ 行是网格 $i$ 的位置编码。

$$f_{\text{pos}} = E_{\text{pos}}[\text{vehicle\_location}]$$

**参数量**：$400 \times 16 = 6400$ 参数

**位置编码的学习**

位置编码是从零初始化的，然后通过反向传播与其他参数一起学习。这样，模型可以自动学到哪些位置特征对调度决策最重要。

#### 3.3.4 日期编码 (Day Embedding)

**为什么需要日期编码？**

网约车的调度模式在不同的日期有显著差异：

- **工作日 (Monday-Friday)**：早晚高峰，订单集中在商业区和住宅区之间
- **周末 (Saturday-Sunday)**：全天流量分散，订单更多集中在娱乐和购物区域

因此，我们添加一个日期编码，将当前的日期（星期几）作为额外的上下文。

**日期编码的实现**

$$f_{\text{day}} = \text{Embedding}(\text{day\_of\_week})$$

其中：
- $\text{day\_of\_week} \in \{0, 1, ..., 6\}$ 代表星期一到星期日
- Embedding 层将日期映射到一个 8 维的向量

从数学角度，Embedding 层是一个矩阵 $E_{\text{day}} \in \mathbb{R}^{7 \times 8}$。

$$f_{\text{day}} = E_{\text{day}}[\text{day\_of\_week}]$$

**参数量**：$7 \times 8 = 56$ 参数

**日期编码的学习**

类似于位置编码，日期编码也是从零初始化的，然后通过反向传播学习。模型会自动学到工作日和周末的调度模式差异。

#### 3.3.5 融合网络 (Fusion Network)

**特征拼接**

将全局特征、位置编码、日期编码进行拼接：

$$f_{\text{concat}} = [f_{\text{global}}, f_{\text{pos}}, f_{\text{day}}]$$

$$f_{\text{concat}} \in \mathbb{R}^{32 + 16 + 8 = 56}$$

**融合网络的架构**

融合网络是一个多层感知机，用于学习这三种特征之间的复杂交互：

```
第 0 层（输入）:
  f_concat ∈ ℝ^56

第 1 层（线性 + 激活）:
  线性变换: ℝ^56 → ℝ^128
    z_1 = f_concat @ W_1 + b_1
    其中 W_1 ∈ ℝ^(56×128), b_1 ∈ ℝ^128

  ReLU 激活:
    h_1 = ReLU(z_1) = max(0, z_1)

  输出: h_1 ∈ ℝ^128

第 2 层（Dropout）:
  Dropout(p=0.2):
    h_1' = Dropout(h_1, p=0.2)

  含义：在训练时，以 20% 的概率随机将 h_1 中的元素设为 0

  输出: h_1' ∈ ℝ^128

第 3 层（线性 + 激活）:
  线性变换: ℝ^128 → ℝ^64
    z_2 = h_1' @ W_2 + b_2
    其中 W_2 ∈ ℝ^(128×64), b_2 ∈ ℝ^64

  ReLU 激活:
    h_2 = ReLU(z_2)

  输出: h_2 ∈ ℝ^64

最终输出:
  f_fusion ∈ ℝ^64
```

**参数统计**：
- 第 1 层：$56 \times 128 + 128 = 7296$ 参数
- 第 2 层（Dropout）：0 参数
- 第 3 层：$128 \times 64 + 64 = 8256$ 参数
- 总计：**15552 参数** $\approx 16K$ 参数

**融合网络的设计原则**

1. **逐步降维**：从 56 维 → 128 维 → 64 维，其中 128 维是一个瓶颈，迫使网络学习最重要的特征
2. **非线性激活**：ReLU 激活函数引入非线性，使网络能够学习复杂的特征交互
3. **正则化**：Dropout 层防止过拟合，提高模型的泛化能力

---

### 3.2.4 Dueling DQN 头

#### 4.4.1 标准 DQN 的局限性

**标准 DQN 的工作流程**

标准的 DQN 直接从融合特征预测每个动作的 Q 值：

$$Q_{\text{standard}}(s, a) = \text{MLP}(f_{\text{fusion}}) \rightarrow [Q_1, Q_2, ..., Q_{179}]$$

其中 MLP 是一个多层感知机，输入是融合特征 $f_{\text{fusion}} \in \mathbb{R}^{64}$，输出是 179 个 Q 值。

**标准 DQN 的问题**

问题 1：**学习困难**

当多个动作具有相似的 Q 值时，小的噪声可能导致大的策略变化。例如：

假设某个状态下，三个动作的真实 Q 值为 $[10.0, 10.1, 10.2]$，即它们的优势分别为 $[0, 0.1, 0.2]$。但由于网络的噪声，模型可能预测为 $[10.5, 9.9, 10.1]$，导致策略完全改变。

问题 2：**收敛缓慢**

模型需要学到精确的 Q 值差异才能做出正确决策。这意味着：
- 需要更多的训练样本
- 训练过程中的波动更大
- 收敛速度更慢

问题 3：**不稳定性**

当环境中存在随机性时（例如，订单的随机生成），Q 值的估计会产生较大的方差。标准 DQN 对这种方差的处理能力较弱。

#### 4.4.2 Dueling DQN 的改进思想

**核心思想：分离两个问题**

Dueling DQN 的核心思想是将 Q 值分解为两部分：

$$Q(s, a) = V(s) + A(s, a)$$

其中：
- $V(s) \in \mathbb{R}$ 是**值函数**，代表状态 $s$ 本身的好坏程度，与具体的动作无关
- $A(s, a) \in \mathbb{R}$ 是**优势函数**，代表在状态 $s$ 下选择动作 $a$ 相对于其他动作的优势

**分解的直观理解**

想象你在一个餐厅选择菜品：
- $V(s)$：这个餐厅的整体质量（好的餐厅 vs 差的餐厅）
- $A(s, a)$：某道菜相对于这个餐厅其他菜品的优势（这道菜是特色菜 vs 普通菜）

总体来说，在一个好的餐厅选择一道特色菜，是一个很好的决策。而在一个差的餐厅选择再好的特色菜，也不是很好的决策。

**数学表达**

标准的 Dueling DQN 公式为：

$$Q(s, a) = V(s) + A(s, a) - \frac{1}{|A|}\sum_{a'} A(s, a')$$

最后一项 $\frac{1}{|A|}\sum_{a'} A(s, a')$ 是所有动作的平均优势。这一项的作用是**确保可识别性** (identifiability)。

**为什么需要减去平均优势？**

如果我们直接使用 $Q(s, a) = V(s) + A(s, a)$，那么存在一个问题：给定任何 $Q$ 值，我们可以通过调整 $V(s)$ 和 $A(s, a)$ 的值来得到相同的 $Q$ 值。例如：

$$Q(s, a) = V(s) + A(s, a) = (V(s) + c) + (A(s, a) - c)$$

对于任何常数 $c$，这两种分解都给出相同的 $Q$ 值。这种**非唯一性**会导致：
- 训练不稳定
- 难以学到正确的 $V$ 和 $A$

为了解决这个问题，我们引入约束条件：

$$\sum_{a'} A(s, a') = 0$$

即所有动作的优势之和为 0。这个约束确保了 $V$ 和 $A$ 的**唯一性**。

通过减去平均优势，我们强制执行这个约束：

$$Q(s, a) = V(s) + \left(A(s, a) - \frac{1}{|A|}\sum_{a'} A(s, a')\right)$$

现在，$\sum_{a'} (A(s, a') - \frac{1}{|A|}\sum_{a''} A(s, a'')) = 0$ 自动满足。

#### 4.4.3 Dueling DQN 的详细实现

**值流 (Value Stream)**

值流用于学习状态的内在价值：

```
第 0 层（输入）:
  f_fusion ∈ ℝ^64

第 1 层（线性 + 激活）:
  线性变换: ℝ^64 → ℝ^128
    z_v = f_fusion @ W_v^(1) + b_v^(1)
    其中 W_v^(1) ∈ ℝ^(64×128), b_v^(1) ∈ ℝ^128

  ReLU 激活:
    h_v = ReLU(z_v)

  输出: h_v ∈ ℝ^128

第 2 层（线性）:
  线性变换: ℝ^128 → ℝ^1
    V(s) = h_v @ W_v^(2) + b_v^(2)
    其中 W_v^(2) ∈ ℝ^(128×1), b_v^(2) ∈ ℝ

  输出: V(s) ∈ ℝ (标量)
```

**参数统计**：
- 第 1 层：$64 \times 128 + 128 = 8320$ 参数
- 第 2 层：$128 \times 1 + 1 = 129$ 参数
- 总计：**8449 参数**

**优势流 (Advantage Stream)**

优势流用于学习每个动作相对于其他动作的优势：

```
第 0 层（输入）:
  f_fusion ∈ ℝ^64

第 1 层（线性 + 激活）:
  线性变换: ℝ^64 → ℝ^128
    z_a = f_fusion @ W_a^(1) + b_a^(1)
    其中 W_a^(1) ∈ ℝ^(64×128), b_a^(1) ∈ ℝ^128

  ReLU 激活:
    h_a = ReLU(z_a)

  输出: h_a ∈ ℝ^128

第 2 层（线性）:
  线性变换: ℝ^128 → ℝ^179
    A(s, a) = h_a @ W_a^(2) + b_a^(2)
    其中 W_a^(2) ∈ ℝ^(128×179), b_a^(2) ∈ ℝ^179

  输出: A(s, a) ∈ ℝ^179 (每个动作一个优势值)
```

**参数统计**：
- 第 1 层：$64 \times 128 + 128 = 8320$ 参数
- 第 2 层：$128 \times 179 + 179 = 22912 + 179 = 23091$ 参数
- 总计：**31411 参数**

**Q 值聚合**

最后，我们将值函数和优势函数聚合为 Q 值：

$$Q(s, a) = V(s) + \left(A(s, a) - \frac{1}{179}\sum_{a'=1}^{179} A(s, a')\right)$$

在代码中，这可以表示为：

```python
# V(s) ∈ ℝ^(B×1)
V = value_stream(f_fusion)  # shape: (B, 1)

# A(s, a) ∈ ℝ^(B×179)
A = advantage_stream(f_fusion)  # shape: (B, 179)

# 计算平均优势 ∈ ℝ^(B×1)
A_mean = A.mean(dim=1, keepdim=True)  # shape: (B, 1)

# 聚合 Q 值 ∈ ℝ^(B×179)
Q = V + (A - A_mean)  # shape: (B, 179)
```

**Dueling DQN 头的完整参数统计**

- 值流：8449 参数
- 优势流：31411 参数
- 总计：**39860 参数** $\approx 40K$ 参数

#### 4.4.4 Dueling DQN 的优势分析

**优势 1：学习稳定性更高**

通过分离值函数和优势函数，模型可以独立地学习每个部分，从而提高学习的稳定性。

**优势 2：收敛速度更快**

值函数的信号会被所有动作共享，这意味着模型可以从更多的样本中学习，从而加速收敛。

**优势 3：对多个相似动作的处理更好**

当多个动作具有相似的 Q 值时，Dueling DQN 可以通过优势函数来区分它们，而不需要精确的 Q 值差异。

**优势 4：更好的泛化能力**

通过分离值函数和优势函数，模型学到的特征更加通用，因此在不同的环境中具有更好的泛化能力。

---

### 3.2.5 模型的完整前向传播

让我们总结一下整个模型的前向传播过程：

**输入**：状态 $s_t = (g_1^{\text{orders}}, ..., g_{400}^{\text{orders}}, g_1^{\text{idle}}, ..., g_{400}^{\text{idle}}, g_1^{\text{busy}}, ..., g_{400}^{\text{busy}}, \text{time\_sin}, \text{time\_cos})$

**第 1 步：MGCN 处理**

输入维度：$(B, 400, 5)$

```
邻接图分支：H_neighbor^(2) ∈ ℝ^(B×400×32)
POI 图分支：H_poi^(2) ∈ ℝ^(B×400×32)
融合：H^(2) ∈ ℝ^(B×400×32)
```

**第 2 步：全局池化**

```
注意力池化：f_global ∈ ℝ^(B×32)
```

**第 3 步：位置和日期编码**

```
位置编码：f_pos ∈ ℝ^(B×16)
日期编码：f_day ∈ ℝ^(B×8)
```

**第 4 步：特征拼接和融合**

```
拼接：f_concat ∈ ℝ^(B×56)
融合网络：f_fusion ∈ ℝ^(B×64)
```

**第 5 步：Dueling DQN 头**

```
值流：V(s) ∈ ℝ^(B×1)
优势流：A(s, a) ∈ ℝ^(B×179)
Q 值：Q(s, a) ∈ ℝ^(B×179)
```

**输出**：Q 值 $Q(s, a) \in \mathbb{R}^{B \times 179}$

### 3.2.6 模型的总体参数统计

| 组件 | 参数量 |
|------|--------|
| MGCN（邻接图分支）| 4736 |
| MGCN（POI 图分支）| 4736 |
| MGCN（融合层）| 196 |
| 注意力池化 | 545 |
| 位置编码 | 6400 |
| 日期编码 | 56 |
| 融合网络 | 15552 |
| 值流 | 8449 |
| 优势流 | 31411 |
| **总计** | **72081** |

整个模型约 **72K 参数**，这是一个相对紧凑的模型，适合在资源受限的环境中部署。

---

## 3.3 训练算法

### 3.3.1 深度 Q 网络 (DQN) 基础

#### 3.3.1.1 强化学习的理论基础与问题建模

在强化学习框架中，我们将网约车调度问题建模为一个**马尔可夫决策过程** (Markov Decision Process, MDP)。MDP 由五元组 $\langle \mathcal{S}, \mathcal{A}, P, R, \gamma \rangle$ 定义，其中：

- $\mathcal{S}$：**状态空间**，包含订单分布、车辆位置、时间信息等
- $\mathcal{A}$：**动作空间**，表示将空闲车辆调度到特定热点网格的决策
- $P(s'|s, a)$：**状态转移概率**，描述执行动作 $a$ 后从状态 $s$ 转移到 $s'$ 的概率
- $R(s, a, s')$：**奖励函数**，在状态转移时给予的反馈信号
- $\gamma \in [0, 1]$：**折扣因子**，用于平衡近期和远期奖励

强化学习的目标是找到**最优策略** $\pi^*$，使得从初始状态出发的**累积奖励期望最大化**：

$$J(\pi) = \mathbb{E}_{\tau \sim \pi} \left[\sum_{t=0}^{T} \gamma^t r_t\right]$$

其中 $\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \ldots, s_T)$ 是一条**轨迹** (trajectory)，表示从初始状态到终止状态的完整交互序列。

**关键假设**：
- 环境满足**马尔可夫性质**：未来的状态只取决于当前状态和动作，不依赖于历史
- 状态空间和动作空间是**有限的**
- 环境是**完全可观测的**（可以直接获得完整的状态信息）

#### 3.3.1.2 动作值函数与贝尔曼方程

**动作值函数** (Q-function) 是强化学习中的核心概念，定义为：

$$Q^\pi(s, a) = \mathbb{E}_{\tau \sim \pi} \left[\sum_{t=0}^{\infty} \gamma^t r_t \mid s_0 = s, a_0 = a\right]$$

这个函数表示：**在状态 $s$ 采取动作 $a$，然后按照策略 $\pi$ 行动，所获得的期望累积折扣奖励**。

**最优动作值函数** $Q^*(s, a)$ 满足**贝尔曼最优性方程** (Bellman Optimality Equation)：

$$Q^*(s, a) = \mathbb{E}_{s' \sim P(\cdot|s,a)} \left[r(s, a, s') + \gamma \max_{a'} Q^*(s', a')\right]$$

这个方程的含义是：
- **当前状态-动作对的最优值** = **即时奖励** + **下一个状态的最优值（折扣后）**
- 这是一个**递归关系**，允许我们通过迭代来求解最优值函数

**具体的数学展开**：

假设我们在状态 $s$ 采取动作 $a$，获得奖励 $r(s, a, s')$，进入状态 $s'$。根据贝尔曼方程：

$$Q^*(s, a) = r(s, a, s') + \gamma \max_{a'} Q^*(s', a')$$

其中：
- $r(s, a, s')$：**即时奖励**，是我们立即获得的反馈
- $\gamma \max_{a'} Q^*(s', a')$：**未来奖励的折扣期望**，表示从 $s'$ 出发采取最优动作能获得的期望奖励（乘以折扣因子 $\gamma$）

**关键参数的含义与设置**：

| 参数 | 值 | 含义 | 设置理由 |
|------|-----|------|---------|
| $\gamma$ | 0.99 | 折扣因子 | 较大的值（接近 1）表示重视长期收益，适合出租车调度这类需要长期规划的任务 |
| $r(s,a,s')$ | 根据阶段 | 即时奖励 | 根据订单生命周期的不同阶段（匹配、完成、取消）给予不同的奖励 |

**最优策略的获得**：

一旦我们有了最优的 Q 值函数 $Q^*(s, a)$，最优策略就可以通过**贪心策略**获得：

$$\pi^*(s) = \arg\max_a Q^*(s, a)$$

这意味着在每个状态，我们选择具有最高 Q 值的动作。

#### 3.3.1.3 DQN 的核心思想与网络参数化

**问题的转化**：

直接求解贝尔曼方程对于大规模问题是不可行的（例如，我们的状态空间有 $400 \times 5 = 2000$ 个维度，远超传统表格方法的能力）。DQN 的创新思想是：

**使用深度神经网络来参数化 Q 函数**。

具体地，我们使用一个参数为 $\theta$ 的深度神经网络 $Q(s, a; \theta)$ 来近似最优动作值函数 $Q^*(s, a)$：

$$Q(s, a; \theta) \approx Q^*(s, a)$$

这样，问题就转化为：**通过调整参数 $\theta$，使得网络输出的 Q 值尽可能接近真实的最优 Q 值**。

**有监督学习的构造**：

虽然我们无法直接知道真实的 $Q^*(s, a)$，但我们可以使用贝尔曼方程来构造一个**有监督学习问题**。

对于每条交互经验 $(s, a, r, s', \text{done})$，我们定义**TD 目标** (Temporal Difference Target)：

$$y = r + \gamma \max_{a'} Q(s', a'; \theta^-) \cdot (1 - \text{done})$$

这里的关键点：

1. **$r$**：当前获得的即时奖励
2. **$\gamma \max_{a'} Q(s', a'; \theta^-)$**：使用**目标网络** $\theta^-$ 来估计下一个状态的最优值（这里用 $\max$ 是因为我们假设在 $s'$ 会采取最优动作）
3. **$(1 - \text{done})$**：当 $\text{done} = 1$（回合结束）时，没有下一个状态，所以 TD 目标仅为 $r$

**损失函数**：

DQN 通过最小化以下**均方误差 (MSE) 损失函数**来学习参数 $\theta$：

$$\mathcal{L}(\theta) = \mathbb{E}_{(s,a,r,s',\text{done}) \sim \mathcal{D}} \left[\left(y - Q(s, a; \theta)\right)^2\right]$$

展开后为：

$$\mathcal{L}(\theta) = \mathbb{E}_{(s,a,r,s',\text{done}) \sim \mathcal{D}} \left[\left(r + \gamma \max_{a'} Q(s', a'; \theta^-) \cdot (1 - \text{done}) - Q(s, a; \theta)\right)^2\right]$$

这个损失函数的含义：
- **最小化 TD 误差的平方**
- TD 误差 = 目标值 - 预测值
- 当网络的预测 $Q(s, a; \theta)$ 接近 TD 目标 $y$ 时，损失最小

**梯度下降更新**：

使用梯度下降法更新参数 $\theta$：

$$\theta \leftarrow \theta - \alpha \nabla_\theta \mathcal{L}(\theta)$$

其中 $\alpha$ 是**学习率**（在我们的实现中为 $1 \times 10^{-4}$）。

#### 3.3.1.4 目标网络的必要性与稳定性分析

**问题：自举与目标移动**

如果我们在计算 TD 目标和计算当前 Q 值时使用**同一个网络**，会产生以下问题：

$$y = r + \gamma \max_{a'} Q(s', a'; \theta) \quad \text{（同一个网络）}$$

$$\mathcal{L}(\theta) = (y - Q(s, a; \theta))^2$$

这会导致：
1. **目标依赖于要优化的参数**：当我们更新 $\theta$ 时，$y$ 也会改变，这称为**目标移动** (moving target)
2. **自举问题**：Q 值的估计依赖于另一个 Q 值的估计，形成"自我引用"
3. **训练不稳定**：梯度可能指向错误的方向，导致参数更新不稳定

**解决方案：目标网络**

DQN 的关键创新是引入一个**独立的目标网络** $Q(\cdot; \theta^-)$，其参数 $\theta^-$ 与主网络 $\theta$ **分离**：

$$y = r + \gamma \max_{a'} Q(s', a'; \theta^-) \quad \text{（目标网络，参数固定）}$$

$$\mathcal{L}(\theta) = (y - Q(s, a; \theta))^2 \quad \text{（主网络，参数更新）}$$

**目标网络的作用**：

| 作用 | 解释 |
|------|------|
| **稳定性** | 目标 $y$ 在多个训练步骤内保持相对稳定（因为 $\theta^-$ 不频繁更新），减少了目标移动的问题 |
| **收敛性** | 有利于梯度下降的收敛，因为目标是相对固定的 |
| **方差减少** | 减少由于参数变化导致的目标方差，使训练更稳定 |

**目标网络的更新机制**：

目标网络定期从主网络复制参数，使用**硬更新**策略：

$$\theta^- \leftarrow \theta \quad \text{（每 } C \text{ 个训练步骤）}$$

在我们的实现中，$C = 1000$，即每 1000 个训练步骤更新一次目标网络。

**更新频率的影响**：
- **更新频率太高**（如每步都更新）：目标网络失去稳定性，效果接近没有目标网络
- **更新频率太低**（如从不更新）：目标网络的参数过时，与主网络差异太大
- **合适的频率**（如 1000 步）：在稳定性和及时性之间取得平衡

#### 3.3.1.5 经验回放的必要性与作用

**问题：样本相关性与训练不稳定**

如果我们按照与环境交互的顺序直接使用样本进行训练：

1. **时间相关性**：连续的样本 $(s_t, a_t, r_t, s_{t+1})$ 和 $(s_{t+1}, a_{t+1}, r_{t+1}, s_{t+2})$ 之间有很强的相关性
2. **样本效率低**：相邻样本提供的信息冗余，浪费计算资源
3. **参数更新不稳定**：相关样本会导致参数更新方向不稳定，甚至导致灾难性遗忘 (catastrophic forgetting)

例如，假设一个车辆连续执行了 10 个相似的调度动作，都获得了正奖励。如果我们按顺序使用这 10 个样本进行训练，网络会过度学习这种特定的模式，而忽视其他可能更优的策略。

**解决方案：经验回放**

DQN 采用**经验回放** (Experience Replay) 机制：

1. **存储经验**：将交互过程中产生的每条经验 $(s, a, r, s', \text{done})$ 存储在一个**回放缓冲区** $\mathcal{D}$ 中
2. **随机采样**：训练时，从缓冲区中**随机采样** mini-batch，而不是按顺序使用
3. **打破相关性**：随机采样使得训练数据近似**独立同分布** (i.i.d.)

**经验回放缓冲区的设计**：

```
回放缓冲区 D:
  容量: |D| = 50,000
  结构: 环形缓冲区 (circular buffer)
  替换策略: FIFO (First-In-First-Out)

当缓冲区满时:
  新经验 → 替换最旧的经验
  这样保证缓冲区始终包含最近的 50,000 条经验
```

**经验回放的优势**：

| 优势 | 解释 |
|------|------|
| **打破相关性** | 随机采样使得 mini-batch 中的样本来自不同的时间步，减少了时间相关性 |
| **样本重用** | 同一条经验可能被多次采样使用，提高了样本效率 |
| **稳定性** | 减少参数更新的方差，使训练更稳定 |
| **数据效率** | 相比于在线学习，经验回放使得样本的利用效率更高 |

**采样过程的伪代码**：

```python
# 训练过程中，每次需要更新网络时
for training_step in range(num_training_steps):
    # 从回放缓冲区中随机采样 mini-batch
    batch = replay_buffer.sample(batch_size=128)

    # 提取 batch 中的各个分量
    states = batch['states']         # shape: (128, 2000)
    actions = batch['actions']       # shape: (128,)
    rewards = batch['rewards']       # shape: (128,)
    next_states = batch['next_states'] # shape: (128, 2000)
    dones = batch['dones']           # shape: (128,)

    # 计算 TD 目标和损失（见 3.3.1.3 部分）
    # ...
```

**缓冲区大小的选择**：

在我们的实现中，选择 $|\mathcal{D}| = 50,000$：
- **较小的缓冲区**（如 10,000）：样本多样性不足，容易过拟合
- **较大的缓冲区**（如 100,000）：内存占用大，采样速度慢
- **50,000 的折衷**：在内存和多样性之间取得平衡

**经验回放与在线学习的对比**：

| 方面 | 经验回放 | 在线学习 |
|------|---------|---------|
| 样本相关性 | 低（随机采样） | 高（顺序采样） |
| 样本效率 | 高（重复使用） | 低（一次性使用） |
| 内存占用 | 高（需要缓冲区） | 低（不需要存储） |
| 训练稳定性 | 好 | 差 |
| 收敛速度 | 快 | 慢 |

### 3.3.2 优先级经验回放 (Prioritized Experience Replay, PER)

#### 3.3.2.1 动机与问题分析

标准的 DQN 从经验回放缓冲区中**均匀随机采样**。这种方法存在以下问题：

**问题 1：样本效率低**

在均匀采样的情况下，所有样本被等同对待。但实际上，不同样本对学习的贡献是不同的：

- **简单样本**（如 TD 误差很小的样本）：模型已经学到了这些样本的特征，继续学习它们效果有限
- **困难样本**（如 TD 误差很大的样本）：模型还没有正确理解这些样本，继续学习它们能显著改进模型

例如，假设缓冲区中有 100 个样本，其中 80 个是简单样本（TD 误差 < 0.1），20 个是困难样本（TD 误差 > 1.0）。如果我们均匀采样，那么每次采样 32 个样本的 mini-batch 中，大约有 25 个简单样本和 6 个困难样本。这意味着我们在浪费大量计算资源来学习已经掌握的知识。

**问题 2：学习不稳定**

均匀采样可能导致模型反复学习简单的样本，而忽视困难样本。这会导致：
- 模型对困难情况的泛化能力差
- 训练过程中的学习曲线不稳定
- 收敛速度慢

**优先级经验回放的核心思想**：优先采样 **TD 误差较大** 的样本，因为这些样本对学习贡献更大。

#### 3.3.2.2 优先级的定义与计算

**优先级的基本定义**：

对于缓冲区中的每个样本 $i$，我们定义其**采样优先级** $p_i$：

$$p_i = \frac{|TD\_error_i|^\alpha}{\sum_j |TD\_error_j|^\alpha}$$

其中：
- $|TD\_error_i| = |r_i + \gamma \max_{a'} Q(s_i', a'; \theta^-) - Q(s_i, a_i; \theta)|$ 是样本 $i$ 的 **TD 误差的绝对值**
- $\alpha \in [0, 1]$ 是一个**优先级强度参数**，控制优先级的影响程度

**参数 $\alpha$ 的含义**：

| $\alpha$ 值 | 采样策略 | 特点 |
|-----------|---------|------|
| $\alpha = 0$ | 均匀采样 | 所有样本概率相等，等同于标准 DQN |
| $0 < \alpha < 1$ | 加权采样 | 高 TD 误差样本有更高概率被采样，但不是绝对的 |
| $\alpha = 1$ | 完全按 TD 误差采样 | 完全由 TD 误差决定采样概率 |

**为什么使用 $\alpha = 0.4$？**

在我们的实现中，选择 $\alpha = 0.4$（而不是 $\alpha = 1$）的原因：

1. **避免过度优先级化**：如果 $\alpha = 1$，某个 TD 误差特别大的样本可能被采样非常多次，导致其他样本被忽视
2. **保留多样性**：$\alpha = 0.4$ 保证了即使 TD 误差较小的样本也有被采样的机会，维持样本多样性
3. **稳定性**：较小的 $\alpha$ 使优先级变化更平滑，训练更稳定

#### 3.3.2.3 重要性采样权重

由于优先级采样会改变数据分布，我们需要通过重要性采样权重进行补偿：

$$w_i = \left(\frac{1}{N \cdot p_i}\right)^\beta$$

其中 $\beta \in [0, 1]$ 是补偿强度参数。在训练中，$\beta$ 从 0.4 逐步增加到 1.0。

#### 3.3.2.4 修正的损失函数

PER 的加权损失函数为：

$$\mathcal{L}(\theta) = \mathbb{E}_{i \sim P(\alpha)} \left[w_i \left(r_i + \gamma \max_{a'} Q(s_i', a'; \theta^-) - Q(s_i, a_i; \theta)\right)^2\right]$$

#### 3.3.2.5 PER 的参数配置

| 参数 | 值 | 说明 |
|------|-----|------|
| $\alpha$ | 0.4 | 优先级强度 |
| $\beta_{\text{start}}$ | 0.4 | 初始补偿权重 |
| $\beta_{\text{end}}$ | 1.0 | 最终补偿权重 |
| 缓冲区大小 | 50,000 | 存储最近 50K 个经验 |

---

### 3.3.3 目标网络与双 DQN

#### 3.3.3.1 高估偏差问题的深入分析

**问题的数学根源**：

标准 DQN 在计算 TD 目标时使用贪心策略：

$$y = r + \gamma \max_{a'} Q(s', a'; \theta^-)$$

这里的 $\max$ 操作会导致一个问题：**高估偏差** (Overestimation Bias)。

**为什么会产生高估偏差？**

假设真实的 Q 值为 $Q^*(s', a')$，而我们的网络估计为 $\hat{Q}(s', a')$。由于网络的不完美性，估计值会有误差：

$$\hat{Q}(s', a') = Q^*(s', a') + \epsilon_a$$

其中 $\epsilon_a$ 是估计误差，通常假设为均值为 0 的随机变量。

当我们对所有动作取最大值时：

$$\max_{a'} \hat{Q}(s', a') = \max_{a'} (Q^*(s', a') + \epsilon_a)$$

由于 $\max$ 操作的非线性性，即使 $\epsilon_a$ 的均值为 0，$\max_a \epsilon_a$ 的期望也是正的！

**直观例子**：

假设某个状态有 3 个动作，真实的 Q 值都是 10，但由于噪声：

- 动作 1 的估计值：$10 + 0.5 = 10.5$
- 动作 2 的估计值：$10 - 0.3 = 9.7$
- 动作 3 的估计值：$10 + 0.8 = 10.8$

取最大值得到 10.8，但真实的最大值应该是 10。这就产生了 0.8 的高估。

**高估偏差的后果**：

1. **错误的策略**：模型可能选择实际上不是最优的动作
2. **训练不稳定**：高估偏差会导致 TD 目标不准确，梯度方向错误
3. **性能下降**：最终得到的策略性能不如预期

#### 3.3.3.2 双 DQN 的改进

双 DQN 使用主网络选择动作，使用目标网络评估 Q 值：

$$y_{\text{DoubleDQN}} = r + \gamma Q(s', \arg\max_{a'} Q(s', a'; \theta); \theta^-)$$

这种分离能够显著减少高估偏差。

#### 3.3.3.3 实现细节

在我们的模型中：

1. **主网络** $Q(\cdot; \theta)$：用于选择动作和计算梯度
2. **目标网络** $Q(\cdot; \theta^-)$：用于计算 TD 目标
3. **更新频率**：每 1000 个训练步骤更新一次目标网络

**目标网络的更新**：
$$\theta^- \leftarrow \theta$$

### 3.3.4 探索策略 (Exploration Strategy)

#### Epsilon-Greedy 策略

为了平衡**探索** (exploration) 和**利用** (exploitation)，我们使用 Epsilon-Greedy 策略：

$$a_t = \begin{cases}
\text{random action} & \text{with probability } \epsilon_t \\
\arg\max_a Q(s_t, a; \theta) & \text{with probability } 1 - \epsilon_t
\end{cases}$$

#### Epsilon 衰减计划

$\epsilon$ 在训练过程中逐渐衰减，使得模型从高探索逐步转向高利用：

$$\epsilon_t = \max(\epsilon_{\text{min}}, \epsilon_{\text{start}} \cdot \gamma_\epsilon^t)$$

其中：

- $\epsilon_{\text{start}} = 0.6$：初始探索概率
- $\epsilon_{\text{min}} = 0.05$：最小探索概率
- $\gamma_\epsilon = 0.95$：每个 episode 的衰减率

**衰减计划的含义**：

- **早期训练**（$\epsilon \approx 0.6$）：高探索，发现多样的策略
- **中期训练**（$\epsilon \approx 0.2$）：平衡探索和利用
- **晚期训练**（$\epsilon \approx 0.05$）：主要利用已学到的策略

这种衰减计划确保了模型既能充分探索，又能逐步收敛到最优策略。

### 3.3.5 训练循环

#### 完整的训练过程

```
初始化主网络和目标网络
初始化经验回放缓冲区

for episode = 1 to NUM_EPISODES:
    state = env.reset()

    for tick = 1 to MAX_TICKS_PER_EPISODE:
        # 1. 选择动作
        if random() < epsilon:
            action = random_action()
        else:
            action = argmax_a Q(state, a)

        # 2. 执行动作，获得反馈
        next_state, reward, done = env.step(action)

        # 3. 存储经验
        replay_buffer.push(state, action, reward, next_state, done)

        # 4. 定期训练（每 TRAIN_EVERY_N_TICKS 步）
        if tick % TRAIN_EVERY_N_TICKS == 0:
            for _ in range(TRAIN_LOOPS_PER_BATCH):
                # 从 PER 缓冲区采样
                batch, indices, weights = replay_buffer.sample(BATCH_SIZE)

                # 计算 TD 目标
                TD_target = batch.reward + gamma * max_a Q(batch.next_state, a, theta_minus)

                # 计算主网络的 Q 值
                Q_main = Q(batch.state, batch.action, theta)

                # 计算加权损失
                TD_error = TD_target - Q_main
                loss = mean(weights * TD_error^2)

                # 反向传播和参数更新
                optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(theta, max_norm=10.0)
                optimizer.step()

                # 更新 PER 优先级
                replay_buffer.update_priorities(indices, abs(TD_error))

        # 5. 定期更新目标网络
        if train_step_count % TARGET_UPDATE_FREQ == 0:
            theta_minus = theta

        state = next_state

    # 6. Episode 结束，衰减 epsilon
    epsilon = max(epsilon_min, epsilon * epsilon_decay)
```

#### 关键参数

| 参数 | 值 | 说明 |
|------|-----|------|
| 学习率 (LR) | $1 \times 10^{-4}$ | Adam 优化器的学习率 |
| 权重衰减 | $1 \times 10^{-5}$ | L2 正则化系数 |
| 折扣因子 ($\gamma$) | 0.99 | 长期奖励的权重 |
| Batch Size | 128 | 每次训练的样本数 |
| 训练频率 | 每 30 个 Ticks | 每 15 分钟训练一次 |
| 每次训练循环数 | 4 | 每次训练迭代 4 个 batch |
| 梯度裁剪 | 10.0 | 防止梯度爆炸 |

#### 优化器配置

我们使用 **Adam 优化器** 配合 **余弦退火学习率调度器**：

$$\text{LR}(t) = \text{LR}_{\text{min}} + \frac{1}{2}(\text{LR}_{\text{max}} - \text{LR}_{\text{min}})(1 + \cos(\pi \cdot t / T))$$

这种调度器能够在训练早期快速下降，然后逐步减小学习率，有助于模型收敛到更好的局部最优点。

### 3.3.6 训练过程的详细分析

#### 训练阶段划分

我们将整个训练过程分为三个阶段，每个阶段有不同的特点和目标：

**第一阶段：探索阶段（Episode 1-100）**

在这个阶段，模型主要进行探索，学习不同动作的基本效果：

- $\epsilon$ 从 0.6 逐步衰减到约 0.3
- 经验回放缓冲区逐渐填充
- 网络参数变化较大，损失函数波动明显
- 关键指标（匹配率、等待时间）可能不稳定

**第二阶段：学习阶段（Episode 101-400）**

模型开始学习有意义的策略，性能逐步提升：

- $\epsilon$ 继续衰减，从 0.3 到约 0.1
- 经验回放缓冲区达到饱和状态
- 网络参数更新速度逐步减缓
- 关键指标开始呈现上升趋势
- 损失函数逐步收敛

**第三阶段：收敛阶段（Episode 401+）**

模型策略趋于稳定，性能指标收敛：

- $\epsilon$ 保持在较低水平（约 0.05）
- 主要进行策略精细化
- 网络参数更新速度很慢
- 关键指标基本稳定
- 可以进行最终的性能评估

#### 训练监测指标

为了全面监测训练过程，我们记录以下关键指标：

**1. 损失函数 (Training Loss)**

$$\text{Loss} = \frac{1}{B} \sum_{i=1}^{B} w_i (r_i + \gamma \max_{a'} Q(s_i', a'; \theta^-) - Q(s_i, a_i; \theta))^2$$

- **含义**：模型预测 Q 值与目标 Q 值的均方误差
- **期望趋势**：总体呈下降趋势，但可能有周期性波动
- **异常情况**：如果损失持续增大，说明学习率过高或数据分布发生剧变

**2. 平均奖励 (Average Reward)**

$$\bar{R}_{\text{episode}} = \frac{1}{T} \sum_{t=0}^{T-1} R_t$$

- **含义**：每个 episode 的平均奖励
- **期望趋势**：总体呈上升趋势，表示策略逐步改善
- **异常情况**：奖励突然下降可能表示环境发生变化

**3. 匹配率 (Match Rate)**

$$\text{Match Rate} = \frac{\text{Number of Matched Orders}}{\text{Total Orders}} \times 100\%$$

- **含义**：被成功匹配的订单占比
- **期望趋势**：应该逐步提高，最终稳定在 85-95%
- **业务意义**：直接影响平台收入和用户体验

**4. 平均等待时间 (Average Waiting Time)**

$$\bar{w}_t = \frac{1}{M} \sum_{i=1}^{M} w_{t,i}$$

其中 $M$ 是匹配的订单数，$w_{t,i}$ 是第 $i$ 个订单的理论等待时间。

- **含义**：乘客平均需要等待的时间
- **期望趋势**：应该逐步降低，最终稳定在 200-300 秒
- **业务意义**：乘客体验的直接体现

**5. 取消率 (Cancellation Rate)**

$$\text{Cancel Rate} = \frac{\text{Number of Cancelled Orders}}{\text{Total Orders}} \times 100\%$$

- **含义**：因超时被取消的订单占比
- **期望趋势**：应该逐步降低，最终稳定在 5-15%
- **业务意义**：反映系统的可靠性

**6. Epsilon 衰减**

$$\epsilon_t = \max(\epsilon_{\text{min}}, \epsilon_{\text{start}} \cdot \gamma_\epsilon^t)$$

- **含义**：探索概率
- **期望趋势**：单调递减，从 0.6 衰减到 0.05
- **监测意义**：确保探索策略按计划执行

#### 训练过程中的常见问题及解决方案

**问题 1：损失函数不收敛**

**症状**：损失函数在训练过程中持续增大或剧烈波动。

**可能原因**：
1. 学习率过高，导致参数更新步长过大
2. 环境中存在非平稳性（如数据分布变化）
3. 网络容量不足，无法学习复杂的 Q 函数

**解决方案**：
1. 降低学习率，从 $1 \times 10^{-4}$ 减少到 $5 \times 10^{-5}$
2. 增加梯度裁剪的阈值，从 10.0 增加到 20.0
3. 增加网络容量，例如增加隐层维度

**问题 2：性能指标不改善**

**症状**：匹配率、等待时间等关键指标在多个 episode 后仍未改善。

**可能原因**：
1. 探索不足，模型陷入局部最优
2. 奖励函数设计不合理，没有正确引导学习
3. 网络架构与问题不匹配

**解决方案**：
1. 增加初始 $\epsilon$ 值，从 0.6 增加到 0.8
2. 重新审视奖励函数的权重设置
3. 尝试不同的网络架构或增加 MGCN 的层数

**问题 3：过拟合现象**

**症状**：训练集上的性能很好，但在测试集或不同时段的数据上性能下降。

**可能原因**：
1. 模型过度学习了特定时段的特征
2. 正则化强度不足
3. 训练数据的时间分布不均匀

**解决方案**：
1. 增加权重衰减系数，从 $1 \times 10^{-5}$ 增加到 $1 \times 10^{-4}$
2. 增加 Dropout 比例，从 0.2 增加到 0.4
3. 使用更多样化的训练数据，包含不同时段和天气条件

#### 梯度流分析

为了确保训练过程的稳定性，我们监测梯度在网络中的流动：

**梯度范数** (Gradient Norm)：

$$\|\nabla_\theta \mathcal{L}\| = \sqrt{\sum_i (\nabla_{\theta_i} \mathcal{L})^2}$$

- **正常范围**：通常在 0.1-10 之间
- **过小**（< 0.01）：可能表示学习信号太弱，网络无法有效学习
- **过大**（> 50）：可能表示梯度爆炸，需要增加梯度裁剪

**梯度方差** (Gradient Variance)：

$$\text{Var}(\nabla_\theta \mathcal{L}) = \frac{1}{n}\sum_{i=1}^{n} (\nabla_{\theta_i} \mathcal{L} - \bar{\nabla})^2$$

- **高方差**：表示不同参数的梯度差异大，可能导致训练不稳定
- **低方差**：表示梯度分布均匀，训练更稳定

---

## 3.4 环境模拟器

### 3.4.1 模拟器的设计

为了训练和评估我们的算法，我们实现了一个**事件驱动的网约车调度模拟器**。模拟器的核心特点：

1. **事件驱动**：基于订单和车辆事件进行推进，而非固定时间步
2. **高效**：能够快速模拟大规模系统（1800 辆车）
3. **真实**：基于真实数据集（北京滴滴数据）

### 3.4.2 模拟器的主要组件

**订单生成器 (OrderGenerator)**：
- 从真实数据集中加载订单
- 按时间顺序生成订单

**车辆管理器 (VehicleManager)**：
- 管理 1800 辆车的状态（空闲、执行任务、调度中）
- 更新车辆位置和状态

**订单匹配器 (OrderMatcher)**：
- 使用 K-D 树进行高效的空间搜索
- 基于距离和时间预测进行匹配

**奖励计算器 (RewardCalculator)**：
- 计算多阶段奖励
- 统计业务指标

---

## 3.5 总结

本章详细介绍了我们提出的网约车调度强化学习模型。主要创新包括：

1. **多阶段奖励函数**：在订单的不同生命周期阶段给予相应反馈，准确反映乘客体验
2. **多图卷积网络 (MGCN)**：融合空间邻接关系和功能相似性，更好地捕捉空间特征
3. **Dueling DQN + PER**：结合现有的强化学习技术，提高训练效率和稳定性
4. **完整的事件驱动模拟器**：基于真实数据进行高效模拟

下一章将介绍实验设置和结果，验证所提方法的有效性。

