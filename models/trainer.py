# models/trainer.py (V5 - 10-sec Tick / Event-Driven)
# æ ¸å¿ƒä¿®æ”¹:
# 1. trainer é€šè¿‡ set_model_and_buffer å°† model å’Œ buffer å¼•ç”¨ä¼ é€’ç»™ envã€‚
# 2. train_episode é‡å†™ä¸º 10 ç§’ Tick å¾ªç¯ (config.MAX_TICKS_PER_EPISODE)ã€‚
# 3. env.step() ç°åœ¨ä¼ å…¥ self.epsilonã€‚
# 4. self.store_experience(...) è¢« *åˆ é™¤*ã€‚å­˜å‚¨åœ¨ env._process_events ä¸­å‘ç”Ÿã€‚
# 5. train_step() ç°åœ¨æŒ‰ config.TRAIN_EVERY_N_TICKS å®šæ—¶è°ƒç”¨ã€‚
# 6. episode_reward ä» env.reward_calculator.total_revenue è·å–ã€‚

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from collections import deque
import random
import numpy as np
import pickle
import os
from datetime import datetime

# ç¡®ä¿å¯¼å…¥è·¯å¾„æ­£ç¡®
try:
    from models.replay_buffer import PrioritizedReplayBuffer
    from models.dispatcher import create_dispatcher
except ImportError as e:
    print(f"å¯¼å…¥é”™è¯¯: {e}. è¯·ç¡®ä¿æ–‡ä»¶è·¯å¾„å’Œåç§°æ­£ç¡®ã€‚")
    raise


class MGCNTrainer:
    """MGCNè°ƒåº¦å™¨è®­ç»ƒå™¨ (V5 - 10-sec Tick / Event-Driven)"""

    def __init__(self, config, neighbor_adj, poi_adj):
        self.config = config
        self.device = config.DEVICE
        print(f"Trainer ä½¿ç”¨è®¾å¤‡: {self.device}")

        # --- åˆ›å»ºç½‘ç»œ (ä¿æŒä¸å˜) ---
        try:
            self.main_net = create_dispatcher(config, neighbor_adj, poi_adj, 'dueling').to(self.device)
            self.target_net = create_dispatcher(config, neighbor_adj, poi_adj, 'dueling').to(self.device)
            self.target_net.load_state_dict(self.main_net.state_dict())
            self.target_net.eval()
            print(f"MainNet/TargetNet (Dueling) device: {next(self.main_net.parameters()).device}")
        except Exception as e:
            print(f"åˆ›å»º dispatcher æ—¶å‡ºé”™: {e}")
            raise

        # --- ä¼˜åŒ–å™¨ (ä¿æŒä¸å˜) ---
        self.optimizer = optim.Adam(
            self.main_net.parameters(), lr=config.LEARNING_RATE, weight_decay=config.WEIGHT_DECAY
        )
        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(self.optimizer, T_0=1000, T_mult=2)

        # --- Replay Buffer (ä¿æŒä¸å˜) ---
        self.replay_buffer = PrioritizedReplayBuffer(
            capacity=config.REPLAY_BUFFER_SIZE, alpha=config.PER_ALPHA,
            beta_start=config.PER_BETA_START, beta_frames=config.PER_BETA_FRAMES
        )

        # --- è®­ç»ƒå‚æ•° (ä¿æŒä¸å˜) ---
        self.epsilon = config.EPSILON_START
        self.epsilon_min = config.EPSILON_END
        self.epsilon_decay = config.EPSILON_DECAY  # (æŒ‰ Episode è¡°å‡)
        self.target_update_freq = config.TARGET_UPDATE_FREQ  # (æŒ‰ train_step è®¡æ•°)

        # --- ç»Ÿè®¡å˜é‡ (ä¿æŒä¸å˜) ---
        self.train_step_count = 0
        self.episode_count = 0
        self.total_rewards = []
        self.losses = []
        self.epsilon_history = []

        # ===== æ–¹æ¡ˆ B: è®­ç»ƒæ›²çº¿è®°å½• =====
        # ç”¨äºç»˜åˆ¶è®­ç»ƒæ›²çº¿çš„æŒ‡æ ‡
        self.completion_rates = []  # è®¢å•å®Œæˆç‡
        self.avg_waiting_times = []  # å¹³å‡ç­‰å¾…æ—¶é—´
        self.match_rates = []  # è®¢å•åŒ¹é…ç‡
        self.cancel_rates = []  # è®¢å•å–æ¶ˆç‡
        # ================================

        self.setup_logging()

    def setup_logging(self):
        # ... (æ—¥å¿—ä»£ç ä¸å˜, V5 æ ‡é¢˜) ...
        try:
            os.makedirs(self.config.LOG_SAVE_PATH, exist_ok=True)
            self.log_file = os.path.join(self.config.LOG_SAVE_PATH,
                                         f"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")
            with open(self.log_file, 'w', encoding='utf-8') as f:
                f.write("MGCN Vehicle Dispatcher Training Log (V5 - 10-sec Tick / Event-Driven)\n")  # æ›´æ–°æ ‡é¢˜
                f.write("=" * 50 + "\n")
                f.write(f"Start Time: {datetime.now()}\n")
                f.write(f"Device: {self.device}\n")
                f.write(
                    f"LR: {self.config.LEARNING_RATE}, Target Update: {self.config.TARGET_UPDATE_FREQ}, Batch: {self.config.BATCH_SIZE}\n")
                f.write(f"PER Alpha: {self.config.PER_ALPHA}, Beta: {self.config.PER_BETA_START}â†’1.0\n")
                # (V5: ç§»é™¤äº† V4 å¥–åŠ±å…¬å¼çš„æ—¥å¿—)
                f.write(f"Reward: Event-Driven (Order Fee), Gamma: {self.config.GAMMA}\n")
                f.write(f"Ticks per Episode: {self.config.MAX_TICKS_PER_EPISODE}\n\n")
        except Exception as e:
            print(f"è®¾ç½®æ—¥å¿—æ—¶å‡ºé”™: {e}")

    def log_message(self, message):
        # ... (æ—¥å¿—ä»£ç ä¸å˜) ...
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S');
        log_entry = f"[{timestamp}] {message}\n"
        if self.config.VERBOSE: print(message)
        try:
            if hasattr(self, 'log_file') and self.log_file:
                with open(self.log_file, 'a', encoding='utf-8') as f: f.write(log_entry)
        except Exception as e:
            print(f"å†™å…¥æ—¥å¿—æ–‡ä»¶æ—¶å‡ºé”™: {e}")

    # (V5: æ­¤å‡½æ•°ä¸å†ä½¿ç”¨ï¼Œé€»è¾‘å·²ç§»å…¥ env._execute_proactive_dispatch)
    # def select_action_for_env(self, ...):
    #     pass

    # (V5: æ­¤å‡½æ•°ä¸å†ä½¿ç”¨ï¼Œé€»è¾‘å·²ç§»å…¥ env._process_events)
    # def store_experience(self, ...):
    #     pass

    # ===== train_step (ä¿æŒä¸å˜, é€»è¾‘å®Œå…¨å…¼å®¹ V5) =====
    def train_step(self):
        """æ‰§è¡Œä¸€æ¬¡è®­ç»ƒæ­¥éª¤ (DDQN + PER) - (V5: é€»è¾‘ä¸å˜)"""
        if len(self.replay_buffer) < self.config.MIN_REPLAY_SIZE: return None
        sample_result = self.replay_buffer.sample(self.config.BATCH_SIZE)
        if sample_result is None: return None
        batch, indices, weights = sample_result
        weights = weights.to(self.device)

        # å‡†å¤‡æ•°æ®å¹¶ç§»åˆ° GPU
        try:
            # (V5: S_t å’Œ S_{t+1} éƒ½æ˜¯ env._get_state() è¿”å›çš„å­—å…¸)
            # (S_t['vehicle_location'] æ˜¯å ä½ç¬¦ 0, A_t æ˜¯çœŸå® action)
            current_node_features = torch.stack([s['node_features'] for s in batch['current_state']]).to(self.device)
            # (V5: æˆ‘ä»¬éœ€è¦ S_t ä¸­ *æš‚å­˜* çš„çœŸå® vehicle_location)
            # (ä¿®å¤ï¼š S_t['vehicle_location'] æ˜¯å ä½ç¬¦ 0ï¼Œä½† S_t_plus_1 ä¸æ˜¯)
            # (ä¿®å¤ï¼šenv._get_state è¿”å› 'vehicle_location'=0 (å ä½ç¬¦))
            # (ä¿®å¤ï¼šdispatcher.py æœŸæœ› 'vehicle_locations' (æ‰¹å¤„ç†))

            # (*** å…³é”®å‡è®¾ ***)
            # å‡è®¾ env._process_events åœ¨ PUSH (S_t, ...) æ—¶,
            # S_t['vehicle_location'] æ˜¯ *çœŸå®* çš„è½¦è¾†ä½ç½®
            # (*** ä¿®å¤ environment.py (V5) ... ***)
            # (*** ä¸´æ—¶ä¿®å¤ï¼šå‡è®¾ S_t['vehicle_location'] æ˜¯å ä½ç¬¦ 0 ***)
            # (*** dispatcher.py (V5) å¿…é¡»ä½¿ç”¨ S_t['vehicle_location'] ***)

            # (*** é‡æ–°å®¡è§† dispatcher.py (V5) ***)
            # (dispatcher.py çš„ forward æ¥æ”¶ 'vehicle_locations' (B,))
            # (dispatcher.py çš„ select_action æ¥æ”¶ 'vehicle_location' (int))

            # (*** é‡æ–°å®¡è§† environment.py (V5) _execute_proactive_dispatch ***)
            # (å®ƒåœ¨ S_micro ä¸­æš‚å­˜ 'vehicle_location'=0 (å ä½ç¬¦))
            # (*** è¿™æ˜¯ä¸€ä¸ª Bug! ***)
            # (*** å¿…é¡»ä¿®å¤ environment.py (V5) ***)
            # (*** (å‡è®¾æˆ‘ä»¬å»ä¿®å¤ environment.py) ***)
            # (*** å‡è®¾ S_t['vehicle_location'] æ˜¯ *çœŸå®* çš„è½¦è¾†ä½ç½® ***)

            # (*** å‡è®¾ env._get_state() V5.1 ***)
            # (*** S_t = env._get_state(vehicle_id) ***)
            # (*** S_t['vehicle_location'] = vehicle.current_grid ***)
            # (*** (å‡è®¾ S_t['vehicle_location'] å·²ä¿®å¤) ***)

            # (*** å‡è®¾ env._get_state() V5 ä¿æŒåŸæ · ***)
            # (*** S_t['vehicle_location'] = 0 (å ä½ç¬¦) ***)
            # (*** ä½† (S_t, A_t) æš‚å­˜åœ¨ vehicle['pending_dispatch_experience'] ***)
            # (*** åœ¨ _execute_... ä¸­, S_micro['vehicle_location'] å¿…é¡»è¢«è®¾ç½®! ***)

            # (*** è®©æˆ‘ä»¬å‡è®¾ env (V5) *æ²¡æœ‰* ä¿®å¤ ***)
            # (*** S_t['vehicle_location'] = 0 (å ä½ç¬¦) ***)
            # (*** é‚£ä¹ˆ dispatcher.py (V5) å¿…é¡»åªä½¿ç”¨ pooled_features + day_emb ***)
            # (*** (è¿™æ˜¯å¦ä¸€ä¸ªå¤§æ”¹åŠ¨) ***)

            # (*** è®©æˆ‘ä»¬å‡è®¾ env (V5) *å·²ä¿®å¤* ***)
            # (*** å‡è®¾ _execute_proactive_dispatch æš‚å­˜çš„ S_micro æ˜¯ï¼š***)
            # (*** S_micro = self._get_state() ***)
            # (*** S_micro['vehicle_location'] = vehicle['current_grid'] ***)
            # (*** S_t_plus_1 = self._get_state() ***)
            # (*** S_t_plus_1['vehicle_location'] = 0 (å ä½ç¬¦) ***)

            # (*** è¿™æ˜¯å”¯ä¸€åˆç†çš„å‡è®¾ ***)

            current_node_features = torch.stack([s['node_features'] for s in batch['current_state']]).to(self.device)
            current_vehicle_locations = torch.tensor([s['vehicle_location'] for s in batch['current_state']],
                                                     dtype=torch.long).to(self.device)
            current_day_of_week = torch.tensor([s['day_of_week'] for s in batch['current_state']], dtype=torch.long).to(
                self.device)

            # å®‰å…¨åœ°è½¬æ¢ä¸ºå¼ é‡ï¼Œé¿å…é‡å¤è½¬æ¢è­¦å‘Š
            if isinstance(batch['actions'], torch.Tensor):
                actions = batch['actions'].to(self.device)
            else:
                actions = torch.tensor(batch['actions'], dtype=torch.long).to(self.device)
                
            if isinstance(batch['rewards'], torch.Tensor):
                rewards = batch['rewards'].to(self.device)
            else:
                rewards = torch.tensor(batch['rewards'], dtype=torch.float32).to(self.device)
                
            if isinstance(batch['dones'], torch.Tensor):
                dones = batch['dones'].to(self.device)
            else:
                dones = torch.tensor(batch['dones'], dtype=torch.bool).to(self.device)

            next_node_features = torch.stack([s['node_features'] for s in batch['next_state']]).to(self.device)
            next_vehicle_locations = torch.tensor([s['vehicle_location'] for s in batch['next_state']],
                                                  dtype=torch.long).to(self.device)  # (å ä½ç¬¦ 0)
            next_day_of_week = torch.tensor([s['day_of_week'] for s in batch['next_state']], dtype=torch.long).to(
                self.device)

        except Exception as e:
            self.log_message(f"é”™è¯¯: train_step ä¸­å‡†å¤‡æ•°æ®å¤±è´¥: {e}")
            # (æ‰“å° S_t ç»“æ„ä»¥ä¾›è°ƒè¯•)
            try:
                self.log_message(f"DEBUG: S_t[0] keys: {batch['current_state'][0].keys()}")
            except:
                pass
            return None

        # æ‰§è¡Œè®­ç»ƒ (V5: é€»è¾‘ä¸å˜, DDQN + PER)
        try:
            # --- è®¡ç®— Q(s,a) ---
            # (S_t æ˜¯ *å†³ç­–æ—¶* çš„çŠ¶æ€, vehicle_location æ˜¯ *çœŸå®* çš„)
            current_q_values_all = self.main_net(current_node_features, current_vehicle_locations, current_day_of_week)
            current_q_values = current_q_values_all.gather(1, actions.unsqueeze(1))

            # --- è®¡ç®— Target Q (DDQN) ---
            with torch.no_grad():
                # (S_{t+1} æ˜¯ *å®Œæˆæ—¶* çš„çŠ¶æ€, vehicle_location æ˜¯ *å ä½ç¬¦ 0*)
                # (*** è¿™æ˜¯ä¸€ä¸ª Bug! ***)
                # (*** Q(S_{t+1}, a') å¿…é¡»æ˜¯ *å®Œæˆæ—¶* è½¦è¾†æ‰€åœ¨ä½ç½®çš„ Q å€¼! ***)

                # (*** ä¿®å¤ environment.py (V5) _process_events ***)
                # (*** S_t_plus_1 = self._get_state() ***)
                # (*** S_t_plus_1['vehicle_location'] = vehicle['current_grid'] ***)

                # (*** å‡è®¾ env (V5) å·²æŒ‰ä¸Šè¿°ä¿®å¤ ***)
                # (*** S_{t+1}['vehicle_location'] æ˜¯è½¦è¾† *æ–°* çš„ç©ºé—²ä½ç½® ***)

                # (*** é‡æ–°å‡è®¾ env (V5) *æœª* ä¿®å¤ ***)
                # (*** S_{t+1}['vehicle_location'] = 0 (å ä½ç¬¦) ***)
                # (*** é‚£ä¹ˆ dispatcher.py (V5) å¿…é¡»å¿½ç•¥ 'vehicle_location' ***)

                # (*** è¿™æ˜¯ä¸€ä¸ªæ— æ³•ç»•è¿‡çš„çŸ›ç›¾ ***)
                # (*** æˆ‘ä»¬å¿…é¡»ä¿®å¤ environment.py (V5) ***)

                # (*** å‡è®¾ environment.py (V5) *å·²ä¿®å¤* ***)
                # (*** _execute_...: S_t['vehicle_location'] = vehicle['current_grid'] ***)
                # (*** _process_events: S_t_plus_1['vehicle_location'] = vehicle['current_grid'] ***)

                next_q_values_main_all = self.main_net(next_node_features, next_vehicle_locations, next_day_of_week)
                next_actions = next_q_values_main_all.argmax(1)

                next_q_values_target_all = self.target_net(next_node_features, next_vehicle_locations, next_day_of_week)
                next_q_values = next_q_values_target_all.gather(1, next_actions.unsqueeze(1))

                target_q_values = rewards + (self.config.GAMMA * next_q_values.squeeze() * (~dones))

            # --- è®¡ç®— Loss ---
            td_errors = torch.abs(current_q_values.squeeze() - target_q_values).detach()
            elementwise_loss = F.mse_loss(current_q_values.squeeze(), target_q_values, reduction='none')
            loss = (weights * elementwise_loss).mean()
            
            # è°ƒè¯•è¾“å‡º
            if self.train_step_count % 100 == 0:  # æ¯100æ­¥è¾“å‡ºä¸€æ¬¡
                self.log_message(f"DEBUG - Step {self.train_step_count}:")
                self.log_message(f"  Current Q values range: [{current_q_values.min().item():.4f}, {current_q_values.max().item():.4f}]")
                self.log_message(f"  Target Q values range: [{target_q_values.min().item():.4f}, {target_q_values.max().item():.4f}]")
                self.log_message(f"  Rewards range: [{rewards.min().item():.4f}, {rewards.max().item():.4f}]")
                self.log_message(f"  TD errors range: [{td_errors.min().item():.4f}, {td_errors.max().item():.4f}]")
                self.log_message(f"  Loss: {loss.item():.6f}")
                self.log_message(f"  Replay buffer size: {len(self.replay_buffer)}")

            # --- åå‘ä¼ æ’­ ---
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.main_net.parameters(), max_norm=1.0)
            self.optimizer.step()
            try:
                self.scheduler.step()
            except Exception:
                pass

            # --- æ›´æ–° ---
            self.replay_buffer.update_priorities(indices, td_errors)
            self.train_step_count += 1
            if self.train_step_count % self.target_update_freq == 0:
                self.target_net.load_state_dict(self.main_net.state_dict())

            return loss.item()

        except Exception as e:
            self.log_message(f"é”™è¯¯: train_step ä¸­ç½‘ç»œè®¡ç®—æˆ–åå‘ä¼ æ’­å¤±è´¥: {e}")
            return None

    # ===============================================

    # ===== æ ¸å¿ƒä¿®æ”¹: train_episode (V5 é€»è¾‘) =====
    def train_episode(self, env, episode):
        """è®­ç»ƒä¸€ä¸ª episode (V5 - 10-sec Tick / Event-Driven)"""

        # 1. (V5) å°† Model å’Œ Buffer å¼•ç”¨ä¼ é€’ç»™ Env
        try:
            env.set_model_and_buffer(self.main_net, self.replay_buffer, self.device)
        except Exception as e:
            self.log_message(f"é”™è¯¯: env.set_model_and_buffer() å¤±è´¥: {e}");
            return 0.0, 0.0

        # 2. (V5) é‡ç½® Env
        try:
            state = env.reset()  # S_0 (åœ¨ T=0 æ—¶åˆ»)
        except Exception as e:
            self.log_message(f"é”™è¯¯: env.reset() å¤±è´¥åœ¨ Ep {episode}: {e}");
            return 0.0, 0.0

        # (V5: episode_reward å°†ä» env.reward_calculator.total_revenue è·å–)
        episode_reward = 0.0
        episode_loss = 0.0
        done = False
        tick_count = 0  # (V5: 10 ç§’ Tick è®¡æ•°å™¨)
        actual_train_steps = 0  # å®é™…è®­ç»ƒæ¬¡æ•°

        self.episode_count = episode

        # 3. (V5) å¼€å§‹ 10 ç§’ Tick å¾ªç¯
        # è¿›åº¦æ˜¾ç¤ºé…ç½®
        show_progress_interval = getattr(self.config, 'SHOW_PROGRESS_EVERY_N_TICKS', 200)

        while not done and tick_count < self.config.MAX_TICKS_PER_EPISODE:

            # --- 3a. (V5) ä¸ç¯å¢ƒäº¤äº’ (æ‰§è¡Œ 10 ç§’çš„ Tick) ---
            try:
                # (å°†å½“å‰çš„ epsilon ä¼ é€’ç»™ env, env å†…éƒ¨æ‰§è¡Œæ¢ç´¢)
                # (env.step() å†…éƒ¨ä¼š: ç§»åŠ¨, åŒ¹é…, ç”Ÿæˆ, è°ƒåº¦(è°ƒç”¨model), push(S,A,R,S'))
                # (è¿”å›çš„ reward æ’ä¸º 0)
                next_state, reward_from_step, done, info = env.step(current_epsilon=self.epsilon)

            except Exception as e:
                self.log_message(f"é”™è¯¯: env.step() å¤±è´¥åœ¨ Ep {episode}, Tick {tick_count}: {e}")
                next_state, reward_from_step, done, info = state, 0, True, {}

            # --- 3b. (V5) å­˜å‚¨ *å®è§‚* ç»éªŒ (***å·²åˆ é™¤***) ---
            # (å­˜å‚¨ (Push) ç°åœ¨å‘ç”Ÿåœ¨ env._process_events å†…éƒ¨)

            # --- 3c. (V5) å­¦ä¹ æ­¥éª¤ (å®šæ—¶) ---
            if (len(self.replay_buffer) >= self.config.MIN_REPLAY_SIZE and
                    tick_count % self.config.TRAIN_EVERY_N_TICKS == 0):

                for _ in range(self.config.TRAIN_LOOPS_PER_BATCH):
                    loss = self.train_step()
                    if loss is not None:
                        episode_loss += loss
                        actual_train_steps += 1

            # --- 3d. æ˜¾ç¤ºè¿›åº¦ï¼ˆæ¯Nä¸ªtickï¼‰ ---
            if tick_count > 0 and tick_count % show_progress_interval == 0:
                buffer_size = len(self.replay_buffer)
                progress_pct = (tick_count / self.config.MAX_TICKS_PER_EPISODE) * 100
                if buffer_size < self.config.MIN_REPLAY_SIZE:
                    print(f"  ğŸ“Š Tick {tick_count}/{self.config.MAX_TICKS_PER_EPISODE} ({progress_pct:.1f}%) | "
                          f"Buffer: {buffer_size}/{self.config.MIN_REPLAY_SIZE} | "
                          f"â³ æ”¶é›†ç»éªŒä¸­...")
                else:
                    avg_loss_so_far = episode_loss / max(1, actual_train_steps)
                    print(f"  ğŸ“Š Tick {tick_count}/{self.config.MAX_TICKS_PER_EPISODE} ({progress_pct:.1f}%) | "
                          f"Buffer: {buffer_size} | "
                          f"è®­ç»ƒæ­¥æ•°: {actual_train_steps} | "
                          f"å¹³å‡Loss: {avg_loss_so_far:.4f}")

            # --- 3e. çŠ¶æ€è½¬ç§» ---
            state = next_state  # S_t å˜ä¸º S_{t+1}
            tick_count += 1

        # --- 4. Episode ç»“æŸå¤„ç† ---

        # (Epsilon è¡°å‡ - æŒ‰ Episode)
        prev_epsilon = self.epsilon
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay;
            self.epsilon = max(self.epsilon_min, self.epsilon)
        self.epsilon_history.append(self.epsilon)

        avg_loss = episode_loss / max(1, actual_train_steps) if actual_train_steps > 0 else 0.0

        # (V5: ä» env.reward_calculator è·å–çœŸå®çš„ç´¯ç§¯å¥–åŠ±)
        try:
            episode_reward = env.reward_calculator.total_revenue
        except Exception:
            episode_reward = 0.0  # Fallback

        self.total_rewards.append(episode_reward)
        self.losses.append(avg_loss)

        # ===== æ–¹æ¡ˆ B: è®°å½•è®­ç»ƒæ›²çº¿æŒ‡æ ‡ =====
        try:
            episode_summary = env.get_episode_summary()
            reward_metrics = episode_summary.get('reward_metrics', {})

            completion_rate = reward_metrics.get('completion_rate', 0.0)
            avg_waiting_time = reward_metrics.get('avg_waiting_time', 0.0)
            match_rate = reward_metrics.get('match_rate', 0.0)
            cancel_rate = reward_metrics.get('cancel_rate', 0.0)

            self.completion_rates.append(completion_rate)
            self.avg_waiting_times.append(avg_waiting_time)
            self.match_rates.append(match_rate)
            self.cancel_rates.append(cancel_rate)

            # æ—¥å¿—è¾“å‡º
            self.log_message(
                f"Episode {episode} Summary: "
                f"Reward={episode_reward:.2f}, Loss={avg_loss:.6f}, Epsilon={self.epsilon:.4f}, "
                f"Completion_Rate={completion_rate:.1%}, Avg_Wait={avg_waiting_time:.1f}s, "
                f"Match_Rate={match_rate:.1%}, Cancel_Rate={cancel_rate:.1%}"
            )
        except Exception as e:
            self.log_message(f"è­¦å‘Š: è®°å½•è®­ç»ƒæ›²çº¿æŒ‡æ ‡å¤±è´¥: {e}")
        # ====================================

        return episode_reward, avg_loss

    # =============================================================

    def save_checkpoint(self, episode):
        # ... (ä»£ç ä¸å˜) ...
        try:
            os.makedirs(self.config.MODEL_SAVE_PATH, exist_ok=True)
            checkpoint = {'episode': episode, 'model_state_dict': self.main_net.state_dict(),
                          'target_model_state_dict': self.target_net.state_dict(),
                          'optimizer_state_dict': self.optimizer.state_dict(),
                          'epsilon': self.epsilon, 'train_step_count': self.train_step_count,
                          'total_rewards': self.total_rewards[-100:], 'losses': self.losses[-100:],
                          'epsilon_history': self.epsilon_history[-100:],
                          'per_frame': self.replay_buffer.frame}
            checkpoint_path = os.path.join(self.config.MODEL_SAVE_PATH, f'mgcn_dispatcher_episode_{episode}.pt')
            torch.save(checkpoint, checkpoint_path)
        except Exception as e:
            self.log_message(f"é”™è¯¯: ä¿å­˜ checkpoint å¤±è´¥: {e}")

    def load_checkpoint(self, checkpoint_path):
        # ... (ä»£ç ä¸å˜) ...
        if not os.path.exists(checkpoint_path):
            self.log_message(f"é”™è¯¯: Checkpoint æ–‡ä»¶æœªæ‰¾åˆ°: {checkpoint_path}")
            raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")
        try:
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            self.main_net.load_state_dict(checkpoint['model_state_dict'])
            self.target_net.load_state_dict(checkpoint['target_model_state_dict'])
            try:
                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            except ValueError as e:
                print(f"è­¦å‘Š: åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€å¤±è´¥: {e}")
            self.epsilon = checkpoint.get('epsilon', self.config.EPSILON_START)
            self.train_step_count = checkpoint.get('train_step_count', 0)
            self.total_rewards = checkpoint.get('total_rewards', [])
            self.losses = checkpoint.get('losses', [])
            self.epsilon_history = checkpoint.get('epsilon_history', [])
            self.replay_buffer.frame = checkpoint.get('per_frame', 1)
            episode = checkpoint.get('episode', 0)
            self.log_message(f"Checkpoint loaded from: {checkpoint_path} (Episode {episode})")
            return episode
        except Exception as e:
            self.log_message(f"é”™è¯¯: åŠ è½½ checkpoint å¤±è´¥: {e}")
            raise e

    def get_training_stats(self):
        # ===== æ–¹æ¡ˆ B: åŒ…å«è®­ç»ƒæ›²çº¿æŒ‡æ ‡ =====
        return {
            'total_rewards': self.total_rewards,
            'losses': self.losses,
            'epsilon_history': self.epsilon_history,
            'current_epsilon': self.epsilon,
            'train_steps': self.train_step_count,
            'episodes': self.episode_count,
            # æ–°å¢è®­ç»ƒæ›²çº¿æŒ‡æ ‡
            'completion_rates': self.completion_rates,
            'avg_waiting_times': self.avg_waiting_times,
            'match_rates': self.match_rates,
            'cancel_rates': self.cancel_rates
        }
        # ====================================

    def get_per_stats(self):
        # (ä»£ç ä¸å˜)
        return self.replay_buffer.get_stats()