# 第 3.3 章 训练算法（续）- 3.3.4 到 3.3 章节结束

## 3.3.4 探索策略 (Exploration Strategy)

### 3.3.4.1 探索与利用的权衡问题

在强化学习中，**探索-利用权衡** (Exploration-Exploitation Trade-off) 是一个基础且重要的问题。

**问题的本质**

在任何时刻，agent 面临两个选择：

1. **探索** (Exploration)：尝试不同的动作，了解环境的未知部分
2. **利用** (Exploitation)：选择已知的最好动作，最大化当前奖励

**探索的必要性**

假设 agent 在早期训练中发现了一个动作能获得 10 的奖励，但实际上还有另一个动作能获得 100 的奖励，只是还没被尝试过。如果 agent 总是选择已知的最好动作，就会永远无法发现更优的动作。

**利用的必要性**

另一方面，如果 agent 总是随机探索，即使已经学到了最优策略，也会不断尝试次优的动作，浪费计算资源。

**平衡的重要性**

最优的策略应该：
- **早期**：进行充分的探索，发现更多可能的策略
- **中期**：逐步减少探索，开始利用已学到的知识
- **晚期**：主要利用已学到的最优策略，偶尔进行探索以应对环境变化

### 3.3.4.2 Epsilon-Greedy 策略

#### 基本思想

Epsilon-Greedy 是最简单也是最常用的探索策略。其思想是：

**以概率 $\epsilon$ 进行随机探索，以概率 $1-\epsilon$ 利用已知的最优动作。**

#### 数学表述

在时刻 $t$，agent 的动作选择为：

$$a_t = \begin{cases}
\text{uniform random from } \mathcal{A} & \text{with probability } \epsilon_t \\
\arg\max_a Q(s_t, a; \theta) & \text{with probability } 1 - \epsilon_t
\end{cases}$$

其中：
- $\epsilon_t$ 是在时刻 $t$ 的探索概率
- $\mathcal{A}$ 是动作空间（在我们的问题中，$|\mathcal{A}| = 179$）
- $Q(s_t, a; \theta)$ 是由神经网络估计的 Q 值

#### 直观理解

想象一个餐厅选择问题：
- **利用**（概率 $1-\epsilon$）：每次都去你最喜欢的餐厅
- **探索**（概率 $\epsilon$）：有时候尝试一家新的餐厅

通过这种平衡，你既能享受最喜欢的餐厅，也有机会发现更好的餐厅。

#### 参数设置

在我们的实现中：
- $\epsilon_{\text{start}} = 0.6$：初始探索概率为 60%
- $\epsilon_{\text{min}} = 0.05$：最小探索概率为 5%

这意味着即使在训练的晚期，agent 仍有 5% 的概率进行随机探索，这有助于应对环境的变化。

### 3.3.4.3 Epsilon 衰减计划

#### 衰减策略

$\epsilon$ 在训练过程中逐渐衰减，使得 agent 从高探索逐步转向高利用。我们使用**指数衰减**策略：

$$\epsilon_t = \max(\epsilon_{\text{min}}, \epsilon_{\text{start}} \cdot \gamma_\epsilon^t)$$

其中：
- $\epsilon_{\text{start}} = 0.6$：初始探索概率
- $\epsilon_{\text{min}} = 0.05$：最小探索概率
- $\gamma_\epsilon = 0.95$：每个 episode 的衰减率
- $t$ 是 episode 的序号

#### 衰减过程的数学分析

**第 0 个 episode**：
$$\epsilon_0 = 0.6$$

**第 10 个 episode**：
$$\epsilon_{10} = 0.6 \times 0.95^{10} \approx 0.6 \times 0.599 \approx 0.359$$

**第 50 个 episode**：
$$\epsilon_{50} = 0.6 \times 0.95^{50} \approx 0.6 \times 0.077 \approx 0.046$$

此时 $\epsilon$ 已经接近最小值 0.05。

**第 100 个 episode**：
$$\epsilon_{100} = \max(0.05, 0.6 \times 0.95^{100}) = 0.05$$

之后 $\epsilon$ 保持在 0.05。

#### 衰减曲线的特点

```
ε 值
 |
0.6 |     ╱╲
    |    ╱  ╲___
0.4 |   ╱       ╲___
    |  ╱            ╲___
0.2 | ╱                 ╲___
    |╱                       ╲___
0.05|___________________________
    |
    └─────────────────────────── Episode
      0  20  40  60  80  100  120
```

**特点**：
- **前 50 个 episodes**：$\epsilon$ 快速下降，从 0.6 降到约 0.05
- **50 个 episodes 之后**：$\epsilon$ 保持在 0.05

这种设计确保了：
1. **早期充分探索**：前 50 个 episodes 中，$\epsilon$ 从 60% 逐步降到 5%，agent 有机会探索各种策略
2. **中后期逐步利用**：50 个 episodes 之后，$\epsilon$ 保持在低水平，agent 主要利用已学到的策略
3. **持续的随机性**：即使在训练晚期，仍有 5% 的概率进行随机探索

### 3.3.4.4 Epsilon-Greedy 的优缺点分析

#### 优点

**1. 简单易实现**
- 只需要一个参数 $\epsilon$
- 实现代码不超过 5 行

**2. 有理论保证**
- 在某些条件下，可以证明 epsilon-greedy 能够收敛到最优策略
- 收敛速度的界已被证明

**3. 直观易理解**
- 易于向非技术人员解释
- 参数的含义清晰

#### 缺点

**1. 探索方式不智能**
- 完全随机的探索，不考虑哪些动作更值得探索
- 例如，如果某个动作的 Q 值为 -100，而最优动作的 Q 值为 10，epsilon-greedy 仍然会以相同的概率探索这个很差的动作

**2. 衰减计划需要手工调整**
- 衰减率 $\gamma_\epsilon$ 需要根据具体问题调整
- 不同的问题可能需要不同的衰减计划

**3. 在高维动作空间中效率低**
- 在我们的问题中，动作空间有 179 个动作
- 随机探索意味着平均需要探索 179 次才能尝试到每个动作
- 这会导致探索阶段很长

### 3.3.4.5 高级探索策略简介

虽然我们主要使用 epsilon-greedy，但还有其他更高级的探索策略值得了解：

#### 1. Boltzmann 探索 (Softmax 策略)

不是二元的选择（随机或贪心），而是按照 Q 值的相对大小来选择动作：

$$P(a|s) = \frac{\exp(Q(s,a) / \tau)}{\sum_{a'} \exp(Q(s,a') / \tau)}$$

其中 $\tau$ 是温度参数。
- **$\tau$ 很大**：所有动作的概率接近，接近随机探索
- **$\tau$ 很小**：高 Q 值的动作概率远大于低 Q 值的动作，接近贪心

**优点**：探索更智能，不会以相同的概率探索很差的动作
**缺点**：计算复杂度更高，需要计算所有动作的 softmax

#### 2. 上置信界 (Upper Confidence Bound, UCB)

选择具有最高"乐观估计"的动作：

$$a_t = \arg\max_a \left( Q(s_t, a) + c \sqrt{\frac{\ln t}{N(a)}} \right)$$

其中：
- $Q(s_t, a)$ 是动作的估计值
- $N(a)$ 是动作 $a$ 被尝试的次数
- $c$ 是探索常数

**优点**：平衡了利用和探索，理论性能有保证
**缺点**：需要跟踪每个动作的尝试次数

#### 3. 熵正则化 (Entropy Regularization)

在损失函数中添加策略熵项，鼓励 agent 保持策略的多样性：

$$\mathcal{L} = \mathcal{L}_{\text{DQN}} - \beta \cdot H(\pi)$$

其中 $H(\pi) = -\sum_a \pi(a) \log \pi(a)$ 是策略的熵。

**优点**：自动平衡探索和利用，无需手工调整衰减计划
**缺点**：增加了损失函数的复杂性

---

## 3.3.5 训练循环与优化

### 3.3.5.1 完整的训练循环

#### 伪代码

```python
# 初始化
main_network = initialize_network()
target_network = copy(main_network)
replay_buffer = PrioritizedReplayBuffer(capacity=50000)
optimizer = Adam(lr=1e-4)
scheduler = CosineAnnealingLR(optimizer, T_max=500)

epsilon = epsilon_start  # 0.6
train_step_count = 0

# 主训练循环
for episode in range(NUM_EPISODES):  # 500 个 episodes
    state = env.reset()
    episode_reward = 0.0

    for tick in range(MAX_TICKS_PER_EPISODE):  # 每个 episode 最多 2880 个 ticks

        # ============ 第 1 步：选择动作 ============
        if random() < epsilon:
            action = random_action()  # 随机探索
        else:
            with torch.no_grad():
                q_values = main_network(state)
                action = argmax(q_values)  # 贪心利用

        # ============ 第 2 步：执行动作 ============
        next_state, reward, done, info = env.step(action)
        episode_reward += reward

        # ============ 第 3 步：存储经验 ============
        replay_buffer.push(state, action, reward, next_state, done)

        # ============ 第 4 步：定期训练 ============
        if tick % TRAIN_EVERY_N_TICKS == 0:  # 每 30 个 ticks 训练一次
            for train_loop in range(TRAIN_LOOPS_PER_BATCH):  # 每次训练 4 个 batch

                # 从 PER 缓冲区采样
                batch, indices, weights = replay_buffer.sample(batch_size=128)

                # 提取 batch 中的各个分量
                states = batch['states']           # shape: (128, 2000)
                actions = batch['actions']         # shape: (128,)
                rewards = batch['rewards']         # shape: (128,)
                next_states = batch['next_states'] # shape: (128, 2000)
                dones = batch['dones']             # shape: (128,)
                weights = batch['weights']         # shape: (128,)

                # 计算 TD 目标（使用目标网络和双 DQN）
                with torch.no_grad():
                    # 使用主网络选择动作
                    next_q_main = main_network(next_states)
                    next_actions = argmax(next_q_main, dim=1)

                    # 使用目标网络评估 Q 值
                    next_q_target = target_network(next_states)
                    next_q_values = next_q_target[range(128), next_actions]

                    # TD 目标
                    td_target = rewards + gamma * next_q_values * (1 - dones)

                # 计算主网络的 Q 值
                q_main = main_network(states)
                q_values = q_main[range(128), actions]

                # 计算 TD 误差
                td_error = td_target - q_values

                # 计算加权损失
                loss = (weights * td_error ** 2).mean()

                # 反向传播和参数更新
                optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(main_network.parameters(), max_norm=10.0)
                optimizer.step()

                # 更新学习率
                scheduler.step()

                # 更新 PER 优先级
                replay_buffer.update_priorities(indices, abs(td_error.detach()))

                train_step_count += 1

        # ============ 第 5 步：定期更新目标网络 ============
        if train_step_count % TARGET_UPDATE_FREQ == 0:  # 每 1000 个训练步更新一次
            target_network.load_state_dict(main_network.state_dict())
            print(f"Target network updated at train_step {train_step_count}")

        state = next_state

        if done:
            break

    # ============ Episode 结束 ============
    # 衰减 epsilon
    epsilon = max(epsilon_min, epsilon * epsilon_decay)

    # 记录 episode 指标
    log_metrics(episode, episode_reward, info)

    # 定期在验证集上评估
    if (episode + 1) % EVAL_EVERY_N_EPISODES == 0:
        eval_reward = evaluate(main_network, eval_env, num_eval_episodes=10)
        print(f"Episode {episode+1}: Train Reward = {episode_reward:.2f}, Eval Reward = {eval_reward:.2f}")
```

### 3.3.5.2 关键参数详解

#### 学习率与学习率调度

**初始学习率**：$\text{LR} = 1 \times 10^{-4}$

这个学习率的选择基于以下考虑：
- **太大**（如 $10^{-3}$）：参数更新步长过大，容易导致训练不稳定，损失函数震荡
- **太小**（如 $10^{-5}$）：参数更新缓慢，收敛速度很慢
- **$10^{-4}$**：在这两者之间取得平衡

**学习率调度**：余弦退火 (Cosine Annealing)

$$\text{LR}(t) = \text{LR}_{\text{min}} + \frac{1}{2}(\text{LR}_{\text{max}} - \text{LR}_{\text{min}})\left(1 + \cos\left(\pi \cdot \frac{t}{T_{\max}}\right)\right)$$

其中：
- $\text{LR}_{\text{max}} = 1 \times 10^{-4}$（初始学习率）
- $\text{LR}_{\text{min}} = 1 \times 10^{-5}$（最小学习率）
- $T_{\max} = 500$（总 episodes 数）

**余弦退火的优点**：
1. **平滑衰减**：学习率不是线性衰减，而是按照余弦函数平滑衰减
2. **热重启**：在某些变体中，学习率会周期性地重启，有助于逃离局部最优
3. **理论支持**：已被证明能改善泛化性能

**学习率曲线**：

```
LR
 |
1e-4|     ╱╲
    |    ╱  ╲
    |   ╱    ╲
    |  ╱      ╲
    | ╱        ╲
1e-5|╱__________╲___
    |
    └─────────────────── Episode
      0  100 200 300 400 500
```

#### 权重衰减 (L2 正则化)

权重衰减系数：$\lambda = 1 \times 10^{-5}$

**作用**：防止过拟合，使模型的权重不会过大

**数学表述**：修改的损失函数为：

$$\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{DQN}} + \lambda \sum_i \theta_i^2$$

这鼓励模型学习较小的权重，从而提高泛化能力。

#### 折扣因子

折扣因子：$\gamma = 0.99$

**含义**：未来的奖励相对于当前奖励的折扣程度

**影响分析**：

对于一个 $n$ 步的决策序列，累积奖励为：

$$R = r_0 + \gamma r_1 + \gamma^2 r_2 + ... + \gamma^n r_n$$

- **$\gamma = 0.99$**：
  - $\gamma^{10} \approx 0.904$：10 步后的奖励仍有 90% 的权重
  - $\gamma^{100} \approx 0.366$：100 步后的奖励只有 37% 的权重
  - 这意味着模型关注长期收益，但也会衰减很远的未来

- **$\gamma = 0.9$**：更短期的视角
- **$\gamma = 0.99$**：更长期的视角
- **$\gamma = 0.999$**：极长期的视角

在网约车调度问题中，$\gamma = 0.99$ 是合理的，因为：
1. 调度决策的影响是中期的（数小时）
2. 过于长期的视角可能导致学习困难

#### 训练频率

**训练频率**：每 30 个 ticks 训练一次

**对应关系**：
- 1 个 tick = 30 秒
- 30 个 ticks = 900 秒 = 15 分钟

这意味着**每 15 分钟进行一次模型训练**。

**选择理由**：
1. **不太频繁**：如果每个 tick 都训练，计算成本太高
2. **不太稀疏**：如果 100 个 ticks 才训练一次，学习信号太弱
3. **平衡**：30 个 ticks 的间隔在计算成本和学习效率之间取得平衡

#### 每次训练的循环数

**每次训练循环数**：4

这意味着每次触发训练时，从回放缓冲区中采样 4 个 batch，分别进行训练。

**好处**：
1. **充分利用采样**：不是采样一个 batch 就立即更新，而是用多个 batch 进行多次更新
2. **样本效率**：提高了样本的利用效率
3. **稳定性**：多次更新使得梯度估计更稳定

#### 梯度裁剪

**梯度裁剪阈值**：$\text{max\_norm} = 10.0$

**作用**：防止梯度爆炸

**数学表述**：

如果梯度的范数超过阈值，则按比例缩放梯度：

$$\nabla_\theta \mathcal{L} \leftarrow \frac{\text{max\_norm}}{\|\nabla_\theta \mathcal{L}\|} \cdot \nabla_\theta \mathcal{L}$$

**为什么需要梯度裁剪**：

在某些情况下，梯度可能变得非常大（特别是当 TD 误差很大时），导致参数更新步长过大，训练不稳定。梯度裁剪限制了每次更新的步长。

### 3.3.5.3 批处理 (Batch Processing)

#### Batch 的组成

每个 batch 包含 128 个样本：

```python
batch = {
    'states': (128, 2000),           # 当前状态
    'actions': (128,),               # 采取的动作
    'rewards': (128,),               # 获得的奖励
    'next_states': (128, 2000),      # 下一个状态
    'dones': (128,),                 # 是否终止
    'weights': (128,),               # PER 权重
    'indices': (128,)                # 缓冲区中的索引
}
```

#### Batch Size 的选择

**Batch Size = 128**

**影响分析**：

| Batch Size | 优点 | 缺点 |
|-----------|------|------|
| 32 | 梯度估计方差小，更新频繁 | 计算效率低，噪声大 |
| 64 | 平衡 | 平衡 |
| 128 | 计算效率高，梯度更稳定 | 梯度估计方差略大 |
| 256 | 非常高效 | 需要更多内存 |

在我们的实现中，选择 128 是因为：
1. **GPU 利用率高**：现代 GPU 对 128 的 batch 大小优化很好
2. **梯度稳定**：128 个样本足以获得稳定的梯度估计
3. **内存平衡**：不会占用过多内存

### 3.3.5.4 优化器选择

#### Adam 优化器

我们使用 **Adam** (Adaptive Moment Estimation) 优化器。

**Adam 的更新规则**：

```
第 t 步的参数更新：

1. 计算梯度
   g_t = ∇_θ L(θ)

2. 更新一阶矩估计（指数移动平均的梯度）
   m_t = β_1 * m_{t-1} + (1 - β_1) * g_t

3. 更新二阶矩估计（指数移动平均的梯度平方）
   v_t = β_2 * v_{t-1} + (1 - β_2) * g_t^2

4. 计算偏差修正
   m_hat_t = m_t / (1 - β_1^t)
   v_hat_t = v_t / (1 - β_2^t)

5. 参数更新
   θ_t = θ_{t-1} - α * m_hat_t / (√v_hat_t + ε)
```

**参数说明**：
- $\beta_1 = 0.9$：一阶矩的衰减率
- $\beta_2 = 0.999$：二阶矩的衰减率
- $\alpha = 1 \times 10^{-4}$：学习率
- $\epsilon = 1 \times 10^{-8}$：数值稳定性常数

**Adam 的优点**：

1. **自适应学习率**：为不同的参数自动调整学习率
2. **动量**：利用梯度的历史信息，加速收敛
3. **鲁棒性**：对学习率的选择不太敏感
4. **广泛适用**：在各种深度学习任务中都表现很好

**为什么不用其他优化器**：

| 优化器 | 优点 | 缺点 |
|--------|------|------|
| SGD | 简单，收敛稳定 | 需要手工调整学习率 |
| RMSprop | 自适应学习率 | 不如 Adam 稳定 |
| Adam | 自适应，鲁棒 | 有时过度正则化 |
| AdamW | Adam 的改进版本 | 略复杂 |

我们选择 Adam 是因为它在强化学习中的表现已被广泛验证。

---

## 3.3.6 训练过程的详细分析

### 3.3.6.1 训练的三个阶段

#### 第一阶段：探索阶段（Episode 1-100）

**特点**：

- **$\epsilon$ 值**：从 0.6 逐步衰减到约 0.3
- **网络状态**：初始化后，参数随机分布
- **学习曲线**：波动剧烈，上下起伏

**发生的事情**：

1. **随机探索**：由于 $\epsilon$ 较大，agent 随机探索各种动作
2. **经验积累**：回放缓冲区逐渐填充，从 0 增长到满容量（50,000）
3. **初步学习**：网络开始学习状态和奖励之间的关系
4. **不稳定性**：由于样本相关性强，训练可能不稳定

**指标表现**：

```
匹配率 (%)
 |
85 |
   |        ╱╲    ╱╲
75 |  ╱╲   ╱  ╲  ╱  ╲
   | ╱  ╲ ╱    ╲╱    ╲___
65 |╱     ╲              ╲___
   |
55 |
   |
   └─────────────────────── Episode
     0   20   40   60   80  100
```

**关键指标**：
- 匹配率：50-70%（不稳定）
- 平均等待时间：400-500 秒（较长）
- 取消率：20-30%（较高）

#### 第二阶段：学习阶段（Episode 101-400）

**特点**：

- **$\epsilon$ 值**：从 0.3 逐步衰减到约 0.05
- **网络状态**：参数逐步调整，收敛到有意义的值
- **学习曲线**：总体上升，波动逐步减小

**发生的事情**：

1. **策略改进**：网络学到了有意义的调度策略
2. **样本多样性**：回放缓冲区保持满容量，包含最近 50,000 条经验
3. **稳定学习**：PER 和目标网络使训练更稳定
4. **性能提升**：关键指标逐步改善

**指标表现**：

```
匹配率 (%)
 |
90 |                    ╱╱╱
   |                ╱╱╱
85 |            ╱╱╱
   |        ╱╱╱
80 |    ╱╱╱
   |╱╱╱
75 |
   |
   └─────────────────────── Episode
     100  150  200  250  300  350  400
```

**关键指标**：
- 匹配率：70-90%（逐步提高）
- 平均等待时间：350-250 秒（逐步降低）
- 取消率：15-8%（逐步降低）

#### 第三阶段：收敛阶段（Episode 401-500）

**特点**：

- **$\epsilon$ 值**：保持在 0.05（最小值）
- **网络状态**：参数基本稳定
- **学习曲线**：平坦，围绕一个稳定值波动

**发生的事情**：

1. **策略稳定**：学到的策略基本不再改变
2. **精细调整**：进行微调，寻求最优性能
3. **收敛验证**：确认模型已收敛到最优或接近最优的策略

**指标表现**：

```
匹配率 (%)
 |
92 |                           ╱╱╱╱╱
   |                          ╱
91 |                         ╱
   |                        ╱
90 |                       ╱
   |                      ╱
89 |                     ╱
   |____________________╱
   |
   └─────────────────────── Episode
     400  420  440  460  480  500
```

**关键指标**：
- 匹配率：88-92%（稳定）
- 平均等待时间：240-280 秒（稳定）
- 取消率：6-8%（稳定）

### 3.3.6.2 损失函数的演化

#### 损失函数的含义

损失函数衡量网络的预测与目标的差距：

$$\mathcal{L}(\theta) = \mathbb{E}_{i \sim P(\alpha)} \left[w_i \left(r_i + \gamma \max_{a'} Q(s_i', a'; \theta^-) - Q(s_i, a_i; \theta)\right)^2\right]$$

**含义**：
- **高损失**：网络的预测远离目标，学习空间大
- **低损失**：网络的预测接近目标，网络已学到有用的特征

#### 损失曲线的典型形状

```
损失值
 |
0.5|  ╱╲╱╲╱╲
   | ╱  ╲  ╲  ╲
0.3|╱    ╲  ╲  ╲___
   |      ╲  ╲     ╲___
0.1|       ╲  ╲_______╲___
   |        ╲__________╲____
   |
   └─────────────────────── Episode
     0   100  200  300  400  500
```

**特点**：
1. **早期快速下降**：前 100 个 episodes，损失快速下降
2. **中期缓慢下降**：100-400 个 episodes，损失缓慢下降
3. **晚期平坦**：400+ 个 episodes，损失基本不变

**解释**：
- 早期下降快：因为网络从随机初始化开始学习
- 中期下降慢：因为网络已学到大部分特征，只需微调
- 晚期平坦：因为网络已收敛

#### 异常的损失曲线

**异常 1：损失持续增大**

```
损失值
 |
1.0|                      ╱╱╱
   |                    ╱╱
0.8|                  ╱╱
   |                ╱╱
0.6|              ╱╱
   |            ╱╱
0.4|          ╱╱
   |        ╱╱
0.2|      ╱╱
   |    ╱╱
   |  ╱╱
   |╱╱
   └─────────────────────── Episode
```

**可能原因**：
- 学习率过高
- 环境非平稳
- 网络容量不足

**解决方案**：降低学习率，或增加网络容量

**异常 2：损失震荡剧烈**

```
损失值
 |
0.5| ╱╲  ╱╲  ╱╲  ╱╲  ╱╲
   |╱  ╲╱  ╲╱  ╲╱  ╲╱  ╲
   |
   └─────────────────────── Episode
```

**可能原因**：
- 学习率过高
- Batch size 过小
- PER 参数不当

**解决方案**：降低学习率，增加 batch size，调整 PER 参数

### 3.3.6.3 梯度流分析

#### 梯度范数的监测

梯度范数 (Gradient Norm) 是衡量训练稳定性的重要指标：

$$\|\nabla_\theta \mathcal{L}\| = \sqrt{\sum_i (\nabla_{\theta_i} \mathcal{L})^2}$$

**正常范围**：0.1 - 10

**梯度范数曲线**：

```
梯度范数
 |
10 |     ╱╲╱╲╱╲
   |    ╱  ╲  ╲  ╲
 5 |   ╱    ╲  ╲  ╲___
   |  ╱      ╲  ╲     ╲___
 1 |_╱________╲__╲_______╲___
   |
0.1|
   |
   └─────────────────────── Episode
     0   100  200  300  400  500
```

**特点**：
- 早期较大（1-10）：因为网络参数差异大
- 晚期较小（0.1-1）：因为网络已收敛
- 总体下降趋势：反映了学习的进行

#### 梯度爆炸与梯度消失

**梯度爆炸**（梯度范数 > 50）：

```
梯度范数
 |
100|                        ╱╱╱
   |                      ╱╱
 50|                    ╱╱
   |                  ╱╱
   |                ╱╱
   |              ╱╱
   |            ╱╱
   |          ╱╱
   |        ╱╱
   └─────────────────────── Episode
```

**后果**：参数更新步长过大，训练不稳定

**解决方案**：
1. 增加梯度裁剪阈值（从 10 增加到 20）
2. 降低学习率
3. 增加 batch size

**梯度消失**（梯度范数 < 0.01）：

```
梯度范数
 |
1.0|╱╲
   |  ╲
0.1|   ╲___
   |       ╲___
0.01|          ╲___
   |              ╲___
   |
   └─────────────────── Episode
```

**后果**：参数几乎不更新，学习停止

**解决方案**：
1. 增加学习率
2. 检查网络架构，确保梯度能够反向传播
3. 使用更好的初始化方法

---

## 3.3.7 训练的监测与调试

### 3.3.7.1 关键指标的监测

#### 1. 训练损失 (Training Loss)

**监测频率**：每 10 个训练步骤记录一次

**期望趋势**：总体下降，但可能有周期性波动

**异常情况**：
- 损失持续增大 → 学习率过高
- 损失持续不变 → 学习率过低
- 损失剧烈震荡 → 数据质量问题或超参数不当

#### 2. 平均奖励 (Average Reward)

**监测频率**：每个 episode 记录一次

**期望趋势**：总体上升

**计算方式**：

$$\bar{R}_{\text{episode}} = \frac{1}{T} \sum_{t=0}^{T-1} R_t$$

其中 $T$ 是 episode 的长度，$R_t$ 是第 $t$ 步的奖励。

#### 3. 匹配率 (Match Rate)

**监测频率**：每个 episode 记录一次

**计算方式**：

$$\text{Match Rate} = \frac{\text{Matched Orders}}{\text{Total Orders}} \times 100\%$$

**期望范围**：85-95%

#### 4. 平均等待时间 (Average Waiting Time)

**监测频率**：每个 episode 记录一次

**计算方式**：

$$\bar{w}_t = \frac{1}{M} \sum_{i=1}^{M} w_{t,i}$$

**期望范围**：200-300 秒

#### 5. 取消率 (Cancellation Rate)

**监测频率**：每个 episode 记录一次

**计算方式**：

$$\text{Cancel Rate} = \frac{\text{Cancelled Orders}}{\text{Total Orders}} \times 100\%$$

**期望范围**：5-15%

### 3.3.7.2 训练曲线的绘制

#### 训练曲线的组成

一个完整的训练曲线包含多个子图：

```
训练过程监控面板
┌─────────────────────────────────────────────────────────┐
│ 训练损失                │ 平均奖励                      │
│                        │                               │
│ Loss ╱╲╱╲╱╲╱╲        │ Reward        ╱╱╱╱╱╱╱       │
│    ╱  ╲  ╲  ╲╲      │        ╱╱╱╱╱╱╱               │
│   ╱    ╲  ╲  ╲╲___  │   ╱╱╱╱╱                       │
│  ╱      ╲__╲__╲___  │╱╱╱                            │
│                     │                               │
├─────────────────────────────────────────────────────────┤
│ 匹配率                  │ 平均等待时间                  │
│                        │                               │
│ Match %              │ Wait Time  ╲                   │
│    ╱╱╱╱╱╱╱╱╱        │         ╲╱╲╱╲╱╲╱╲          │
│ ╱╱╱                  │        ╱  ╲  ╲  ╲╲___       │
│╱                     │       ╱    ╲__╲__╲___       │
│                     │                               │
├─────────────────────────────────────────────────────────┤
│ 取消率                  │ Epsilon 衰减                  │
│                        │                               │
│ Cancel %            │ Epsilon ╲╲╲╲╲╲╲╲           │
│    ╲╲╲╲╲╲╲╲╲        │        ╲╲╲╲╲╲╲╲           │
│     ╲╲╲╲╲╲╲╲      │       ╱╱╱╱╱╱╱╱╱            │
│    ╱╱╱╱╱╱╱╱╱╱    │      ╱                        │
│   ╱              │                               │
└─────────────────────────────────────────────────────────┘
```

### 3.3.7.3 常见问题及调试方法

#### 问题 1：训练不收敛

**症状**：经过 500 个 episodes 的训练，性能指标仍未改善

**诊断步骤**：

1. **检查损失函数**：
   - 如果损失持续增大 → 学习率过高
   - 如果损失不变 → 学习率过低或网络无法学习

2. **检查梯度**：
   - 梯度范数是否在正常范围内？
   - 是否存在梯度爆炸或消失？

3. **检查奖励信号**：
   - 奖励是否合理？
   - 是否存在奖励稀疏的问题？

**解决方案**：

```
如果损失持续增大：
  ├─ 降低学习率（从 1e-4 到 5e-5）
  ├─ 增加梯度裁剪阈值（从 10 到 20）
  └─ 增加 batch size（从 128 到 256）

如果损失不变：
  ├─ 提高学习率（从 1e-4 到 2e-4）
  ├─ 检查网络架构是否有问题
  └─ 检查数据是否有问题

如果奖励信号弱：
  ├─ 检查奖励函数的权重设置
  ├─ 增加 epsilon 的初始值（从 0.6 到 0.8）
  └─ 减少 epsilon 的衰减率（从 0.95 到 0.98）
```

#### 问题 2：过拟合

**症状**：训练集上的性能很好，但在验证集上性能差

**诊断步骤**：

1. **对比训练集和验证集的性能**：
   - 训练集匹配率：95%，验证集匹配率：75%
   - 差距很大 → 过拟合

2. **检查网络容量**：
   - 网络是否太大？
   - 是否需要正则化？

3. **检查数据**：
   - 训练集和验证集的分布是否不同？
   - 是否存在数据泄露？

**解决方案**：

```
增加正则化：
  ├─ 增加权重衰减（从 1e-5 到 1e-4）
  ├─ 增加 dropout 比例（从 0.2 到 0.4）
  └─ 增加 L1 正则化

减少模型复杂度：
  ├─ 减少网络层数
  ├─ 减少隐层维度
  └─ 使用更小的模型

改进数据：
  ├─ 增加训练数据量
  ├─ 数据增强
  └─ 确保训练集和验证集的分布一致
```

#### 问题 3：训练不稳定

**症状**：损失函数剧烈震荡，性能指标波动很大

**诊断步骤**：

1. **检查学习率**：
   - 学习率是否过高？
   - 是否需要学习率调度？

2. **检查 batch size**：
   - Batch size 是否过小？
   - 梯度估计是否噪声太大？

3. **检查 PER 参数**：
   - $\alpha$ 和 $\beta$ 是否设置合理？
   - 是否导致了样本分布过于偏斜？

**解决方案**：

```
调整学习率：
  ├─ 降低初始学习率
  ├─ 使用学习率调度（如余弦退火）
  └─ 增加学习率预热

增加 batch size：
  ├─ 从 128 增加到 256
  └─ 使用梯度累积

调整 PER 参数：
  ├─ 降低 alpha（从 0.4 到 0.2）
  ├─ 增加 beta 的初始值（从 0.4 到 0.6）
  └─ 检查缓冲区大小是否足够
```

---

## 3.3.8 总结

本节详细介绍了网约车调度强化学习模型的训练算法，主要内容包括：

### 核心算法

1. **DQN 基础**：使用神经网络参数化 Q 函数，通过最小化 TD 误差进行训练
2. **优先级经验回放 (PER)**：优先采样 TD 误差大的样本，提高样本效率
3. **双 DQN**：使用目标网络评估 Q 值，减轻高估偏差
4. **Epsilon-Greedy 探索**：平衡探索和利用，逐步衰减探索概率

### 关键技术

1. **目标网络**：稳定训练，减少目标移动问题
2. **经验回放**：打破样本相关性，提高训练稳定性
3. **梯度裁剪**：防止梯度爆炸
4. **学习率调度**：自动调整学习率，加速收敛

### 训练流程

1. **初始化**：创建主网络、目标网络、回放缓冲区
2. **交互**：与环境交互，收集经验
3. **训练**：从缓冲区采样，更新网络参数
4. **评估**：定期在验证集上评估性能

### 监测与调试

1. **关键指标**：损失函数、平均奖励、匹配率、等待时间、取消率
2. **异常检测**：识别训练过程中的问题
3. **调试方法**：根据问题采取相应的解决方案

这套完整的训练框架确保了模型能够高效、稳定地学习最优的调度策略。

